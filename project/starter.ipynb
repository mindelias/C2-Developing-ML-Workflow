{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy and monitor a machine learning workflow for Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up this notebook\n",
    "\n",
    "Notes about the instance size and kernel setup: this notebook has been tested on\n",
    "\n",
    "1. The `Python 3 (Data Science)` kernel\n",
    "2. The `ml.t3.medium` Sagemaker notebook instance\n",
    "\n",
    "## Data Staging\n",
    "\n",
    "We'll use a sample dataset called CIFAR to simulate the challenges Scones Unlimited are facing in Image Classification. In order to start working with CIFAR we'll need to:\n",
    "\n",
    "1. Extract the data from a hosting service\n",
    "2. Transform it into a usable shape and format\n",
    "3. Load it into a production system\n",
    "\n",
    "In other words, we're going to do some simple ETL!\n",
    "\n",
    "### 1. Extract the data from the hosting service\n",
    "\n",
    "In the cell below, define a function `extract_cifar_data` that extracts python version of the CIFAR-100 dataset. The CIFAR dataaset is open source and generously hosted by the University of Toronto at: https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def extract_cifar_data(url, filename=\"cifar.tar.gz\"):\n",
    "    \"\"\"Extracts the CIFAR-100 dataset and stores it as a gzipped file\n",
    "    \n",
    "    Arguments:\n",
    "    url      -- URL where the dataset is hosted\n",
    "    filename -- full path where the dataset will be written\n",
    "    \"\"\"\n",
    "    # Request the data from the URL using requests.get\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    # Ensure the request was successful\n",
    "    r.raise_for_status()\n",
    "    \n",
    "    # Write the content to a file in binary mode\n",
    "    with open(filename, \"wb\") as file_context:\n",
    "        file_context.write(r.content)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out! Run the following cell and check whether a new file `cifar.tar.gz` is created in the file explorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_cifar_data(\"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Transform the data into a usable shape and format\n",
    "\n",
    "Clearly, distributing the data as a gzipped archive makes sense for the hosting service! It saves on bandwidth, storage, and it's a widely-used archive format. In fact, it's so widely used that the Python community ships a utility for working with them, `tarfile`, as part of its Standard Library. Execute the following cell to decompress your extracted dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open(\"cifar.tar.gz\", \"r:gz\") as tar:\n",
    "    tar.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new folder `cifar-100-python` should be created, containing `meta`, `test`, and `train` files. These files are `pickles` and the [CIFAR homepage](https://www.cs.toronto.edu/~kriz/cifar.html) provides a simple script that can be used to load them. We've adapted the script below for you to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./cifar-100-python/meta\", \"rb\") as f:\n",
    "    dataset_meta = pickle.load(f, encoding='bytes')\n",
    "\n",
    "with open(\"./cifar-100-python/test\", \"rb\") as f:\n",
    "    dataset_test = pickle.load(f, encoding='bytes')\n",
    "\n",
    "with open(\"./cifar-100-python/train\", \"rb\") as f:\n",
    "    dataset_train = pickle.load(f, encoding='bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to explore the datasets\n",
    "import pandas as pd\n",
    "\n",
    "dataset_train.keys()\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As documented on the homepage, `b'data'` contains rows of 3073 unsigned integers, representing three channels (red, green, and blue) for one 32x32 pixel image per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "32*32*3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a simple gut-check, let's transform one of our images. Each 1024 items in a row is a channel (red, green, then blue). Each 32 items in the channel are a row in the 32x32 image. Using python, we can stack these channels into a 32x32x3 array, and save it as a PNG file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Each 1024 in a row is a channel (red, green, then blue)\n",
    "row = dataset_train[b'data'][0]\n",
    "print(row)\n",
    "red, green, blue = row[0:1024], row[1024:2048], row[2048:]\n",
    "\n",
    "# Each 32 items in the channel are a row in the 32x32 image\n",
    "red = red.reshape(32,32)\n",
    "green = green.reshape(32,32)\n",
    "blue = blue.reshape(32,32)\n",
    "\n",
    "# Combine the channels into a 32x32x3 image!\n",
    "combined = np.dstack((red,green,blue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more concise version, consider the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All in one:\n",
    "#Channels are stacked depth-wise to form a 32x32x3 RGB image:\n",
    "test_image = np.dstack((\n",
    "    row[0:1024].reshape(32,32),\n",
    "    row[1024:2048].reshape(32,32),\n",
    "    row[2048:].reshape(32,32)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(test_image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like a cow! Let's check the label. `dataset_meta` contains label names in order, and `dataset_train` has a list of labels for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train[b'fine_labels'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our image has a label of `19`, so let's see what the 19th item is in the list of label names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_meta[b'fine_label_names'][19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! 'cattle' sounds about right. By the way, using the previous two lines we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "print(dataset_meta[b'fine_label_names'][dataset_train[b'fine_labels'][n]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how to check labels, is there a way that we can also check file names? `dataset_train` also contains a `b'filenames'` key. Let's see what we have here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_train[b'filenames'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Taurus\" is the name of a subspecies of cattle, so this looks like a pretty reasonable filename. To save an image we can also do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imsave(\"file.png\", test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your new PNG file should now appear in the file explorer -- go ahead and pop it open to see!\n",
    "\n",
    "Now that you know how to reshape the images, save them as files, and capture their filenames and labels, let's just capture all the bicycles and motorcycles and save them. Scones Unlimited can use a model that tells these apart to route delivery drivers automatically.\n",
    "\n",
    "In the following cell, identify the label numbers for Bicycles and Motorcycles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create the DataFrame from image data (likely shape (N, 3072))\n",
    "df = pd.DataFrame(dataset_train[b'data'])\n",
    "\n",
    "# Add filename column (decode bytes to string)\n",
    "df['filename'] = [name.decode() for name in dataset_train[b'filenames']]\n",
    "\n",
    "# Add fine label index\n",
    "df['label_idx'] = dataset_train[b'fine_labels']\n",
    "\n",
    "# Add actual label names using the label index and metadata\n",
    "label_names = [label.decode() for label in dataset_meta[b'fine_label_names']]\n",
    "df['label'] = df['label_idx'].apply(lambda i: label_names[i])\n",
    "\n",
    "label_names[:6]\n",
    "\n",
    "# Preview\n",
    "# print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Todo: Filter the dataset_train and dataset_meta objects to find the label numbers for Bicycle and Motorcycles\n",
    "\n",
    "# Find the label indexes for 'bicycle' and 'motorcycle'\n",
    "# Decode label names from bytes to string\n",
    "fine_label_names = [name.decode() for name in dataset_meta[b'fine_label_names']]\n",
    "\n",
    "# Find the indexes for 'bicycle' and 'motorcycle'\n",
    "bicycle_idx = fine_label_names.index('bicycle')\n",
    "motorcycle_idx = fine_label_names.index('motorcycle')\n",
    "\n",
    "print(\"Bicycle index:\", bicycle_idx)       # For verification\n",
    "print(\"Motorcycle index:\", motorcycle_idx)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job! We only need objects with label 8 and 48 -- this drastically simplifies our handling of the data! Below we construct a dataframe for you, and you can safely drop the rows that don't contain observations about bicycles and motorcycles. Fill in the missing lines below to drop all other rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct the dataframe\n",
    "df_train = pd.DataFrame({\n",
    "    \"filenames\": dataset_train[b'filenames'],\n",
    "    \"labels\": dataset_train[b'fine_labels'],\n",
    "    \"row\": range(len(dataset_train[b'filenames']))\n",
    "})\n",
    "\n",
    "# Drop all rows from df_train where label is not 8 or 48\n",
    "df_train = df_train[df_train['labels'].isin([bicycle_idx, motorcycle_idx ])]\n",
    "\n",
    "# Decode df_train.filenames so they are regular strings\n",
    "df_train[\"filenames\"] = df_train[\"filenames\"].apply(\n",
    "    lambda x: x.decode(\"utf-8\")\n",
    ")\n",
    "\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    \"filenames\": dataset_test[b'filenames'],\n",
    "    \"labels\": dataset_test[b'fine_labels'],\n",
    "    \"row\": range(len(dataset_test[b'filenames']))\n",
    "})\n",
    "\n",
    "# Drop all rows from df_test where label is not 8 or 48\n",
    "df_test = df_test[df_test['labels'].isin([bicycle_idx, motorcycle_idx ])]\n",
    "\n",
    "# Decode df_test.filenames so they are regular strings\n",
    "df_test[\"filenames\"] = df_test[\"filenames\"].apply(\n",
    "    lambda x: x.decode(\"utf-8\")\n",
    ")\n",
    "print(f\"TRAIN DF: {df_train.head()}\")\n",
    "print(f\"TEST DF: {df_test.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is filtered for just our classes, we can save all our images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./train\n",
    "!mkdir ./test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous sections we introduced you to several key snippets of code:\n",
    "\n",
    "1. Grabbing the image data:\n",
    "\n",
    "```python\n",
    "dataset_train[b'data'][0]\n",
    "```\n",
    "\n",
    "2. A simple idiom for stacking the image data into the right shape\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "np.dstack((\n",
    "    row[0:1024].reshape(32,32),\n",
    "    row[1024:2048].reshape(32,32),\n",
    "    row[2048:].reshape(32,32)\n",
    "))\n",
    "```\n",
    "\n",
    "3. A simple `matplotlib` utility for saving images\n",
    "\n",
    "```python\n",
    "plt.imsave(path+row['filenames'], target)\n",
    "```\n",
    "\n",
    "Compose these together into a function that saves all the images into the `./test` and `./train` directories. Use the comments in the body of the `save_images` function below to guide your construction of the function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def save_images(dataset, path=\"./train/\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    for i in range(len(dataset[b'data'])):\n",
    "        #Grab the image data in row-major form\n",
    "        img = dataset[b'data'][i]\n",
    "    \n",
    "        # Consolidated stacking/reshaping from earlier\n",
    "        target = np.dstack((\n",
    "            img[0:1024].reshape(32, 32),         # Red channel\n",
    "            img[1024:2048].reshape(32, 32),      # Green channel\n",
    "            img[2048:].reshape(32, 32)           # Blue channel\n",
    "        ))\n",
    "        \n",
    "         # 3. Decode filename\n",
    "        filename = dataset[b'filenames'][i].decode()\n",
    "    \n",
    "        # Save the image\n",
    "        plt.imsave(os.path.join(path, filename), target)\n",
    "\n",
    "    \n",
    "    # Return any signal data you want for debugging\n",
    "    return f\"Saved {len(dataset[b'data'])} images to {path}\"\n",
    "\n",
    "## TODO: save ALL images using the save_images function\n",
    "save_images(dataset_train, path=\"./train/\")\n",
    "# save_images(dataset_test, path=\"./test/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load the data\n",
    "\n",
    "Now we can load the data into S3.\n",
    "\n",
    "Using the sagemaker SDK grab the current region, execution role, and bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "Default Bucket: sagemaker-us-west-2-869935066996\n",
      "AWS Region: us-west-2\n",
      "RoleArn: arn:aws:iam::869935066996:role/service-role/AmazonSageMaker-ExecutionRole-20250802T162073\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "\n",
    "session = sagemaker.Session()\n",
    "bucket= \"sagemaker-us-west-2-869935066996\"\n",
    "print(\"Default Bucket: {}\".format(bucket))\n",
    "\n",
    "region = session.boto_region_name\n",
    "print(\"AWS Region: {}\".format(region))\n",
    "\n",
    " \n",
    "role = get_execution_role()\n",
    "print(\"RoleArn: {}\".format(role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this data we can easily sync your data up into S3!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DEFAULT_S3_BUCKET\"] = bucket\n",
    "!aws s3 sync ./train s3://${DEFAULT_S3_BUCKET}/scones-unlimited/train/\n",
    "!aws s3 sync ./test s3://${DEFAULT_S3_BUCKET}/scones-unlimited/test/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! You can check the bucket and verify that the items were uploaded.\n",
    "\n",
    "## Model Training\n",
    "\n",
    "For Image Classification, Sagemaker [also expects metadata](https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html) e.g. in the form of TSV files with labels and filepaths. We can generate these using our Pandas DataFrames from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_metadata_file(df, prefix):\n",
    "    df[\"s3_path\"] = df[\"filenames\"]\n",
    "    df[\"labels\"] = df[\"labels\"].apply(lambda x: 0 if x==8 else 1)\n",
    "    return df[[\"row\", \"labels\", \"s3_path\"]].to_csv(\n",
    "        f\"{prefix}.lst\", sep=\"\\t\", index=False, header=False\n",
    "    )\n",
    "    \n",
    "to_metadata_file(df_train.copy(), \"train\")\n",
    "to_metadata_file(df_test.copy(), \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also upload our manifest files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Upload files\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "s3.upload_file(\n",
    "    Filename='./train.lst',\n",
    "    Bucket=bucket,\n",
    "    Key='scones-unlimited/manifest/train.lst'\n",
    ")\n",
    "s3.upload_file(\n",
    "    Filename='./test.lst',\n",
    "    Bucket=bucket,\n",
    "    Key='scones-unlimited/manifest/test.lst'\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `bucket` and `region` info we can get the latest prebuilt container to run our training job, and define an output location on our s3 bucket for the model. Use the `image_uris` function from the SageMaker SDK to retrieve the latest `image-classification` image below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-869935066996/scones-unlimited/models/image_model\n"
     ]
    }
   ],
   "source": [
    "# Use the image_uris function to retrieve the latest 'image-classification' image \n",
    "from sagemaker import image_uris\n",
    "algo_image = image_uris.retrieve(\n",
    "    framework='image-classification',   # algorithm name\n",
    "    region=boto3.Session().region_name  # current AWS region\n",
    ")\n",
    "s3_output_location = f\"s3://{bucket}/scones-unlimited/models/image_model\"\n",
    "print(s3_output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to create an estimator! Create an estimator `img_classifier_model` that uses one instance of `ml.p3.2xlarge`. Ensure that y ou use the output location we defined above - we'll be referring to that later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type='ml.g4dn.xlarge' \n",
    "instance_count = 1\n",
    "\n",
    "img_classifier_model=sagemaker.estimator.Estimator(\n",
    "    ## TODO: define your estimator options    \n",
    "    image_uri=algo_image,\n",
    "    role=role,\n",
    "    instance_count= 1,\n",
    "    instance_type=instance_type,\n",
    "    output_path=s3_output_location,\n",
    "    base_job_name='scones-unlimited',  \n",
    "    sagemaker_session=session,\n",
    "    use_spot_instances=True,         \n",
    "    max_run=3600,                     \n",
    "    max_wait=3600,                   \n",
    "   \n",
    "    \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also set a few key hyperparameters and define the inputs for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_classifier_model.set_hyperparameters(\n",
    "    image_shape='3,32,32', # TODO: Fill in\n",
    "    num_classes=2, # TODO: Fill in\n",
    "    num_training_samples=5000 # TODO: fill in\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `image-classification` image uses four input channels with very specific input parameters. For convenience, we've provided them below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import Rule, rule_configs\n",
    "from sagemaker.session import TrainingInput\n",
    "model_inputs = {\n",
    "        \"train\": sagemaker.inputs.TrainingInput(\n",
    "            s3_data=f\"s3://{bucket}/scones-unlimited/train/\",\n",
    "            \n",
    "            content_type=\"application/x-image\"\n",
    "        ),\n",
    "        \"validation\": sagemaker.inputs.TrainingInput(\n",
    "            s3_data=f\"s3://{bucket}/scones-unlimited/test/\",\n",
    "            content_type=\"application/x-image\"\n",
    "        ),\n",
    "        \"train_lst\": sagemaker.inputs.TrainingInput(\n",
    "            s3_data=f\"s3://{bucket}/scones-unlimited/manifest/train.lst\",\n",
    "            content_type=\"application/x-image\"\n",
    "        ),\n",
    "        \"validation_lst\": sagemaker.inputs.TrainingInput(\n",
    "            s3_data=f\"s3://{bucket}/scones-unlimited/manifest/test.lst\",\n",
    "            content_type=\"application/x-image\"\n",
    "        )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we can train the model using the model_inputs. In the cell below, call the `fit` method on our model,:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: scones-unlimited-2025-08-07-20-39-54-431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-07 20:39:55 Starting - Starting the training job...\n",
      "2025-08-07 20:40:10 Starting - Preparing the instances for training...\n",
      "2025-08-07 20:40:38 Downloading - Downloading input data......\n",
      "2025-08-07 20:41:44 Downloading - Downloading the training image......\n",
      "2025-08-07 20:42:35 Training - Training image download completed. Training in progress.\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34mNvidia gpu devices, drivers and cuda toolkit versions (only available on hosts with GPU):\u001b[0m\n",
      "\u001b[34mThu Aug  7 20:42:46 2025       \u001b[0m\n",
      "\u001b[34m+-----------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34m| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\u001b[0m\n",
      "\u001b[34m|-----------------------------------------+------------------------+----------------------+\u001b[0m\n",
      "\u001b[34m| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\u001b[0m\n",
      "\u001b[34m| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\u001b[0m\n",
      "\u001b[34m|                                         |                        |               MIG M. |\u001b[0m\n",
      "\u001b[34m|=========================================+========================+======================|\u001b[0m\n",
      "\u001b[34m|   0  Tesla T4                       On  |   00000000:00:1E.0 Off |                    0 |\u001b[0m\n",
      "\u001b[34m| N/A   31C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\u001b[0m\n",
      "\u001b[34m|                                         |                        |                  N/A |\u001b[0m\n",
      "\u001b[34m+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \u001b[0m\n",
      "\u001b[34m+-----------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34m| Processes:                                                                              |\u001b[0m\n",
      "\u001b[34m|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\u001b[0m\n",
      "\u001b[34m|        ID   ID                                                               Usage      |\u001b[0m\n",
      "\u001b[34m|=========================================================================================|\u001b[0m\n",
      "\u001b[34m|  No running processes found                                                             |\u001b[0m\n",
      "\u001b[34m+-----------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34mChecking for nvidia driver and cuda compatibility.\u001b[0m\n",
      "\u001b[34mCUDA Compatibility driver provided.\u001b[0m\n",
      "\u001b[34mProceeding with compatibility check between driver, cuda-toolkit and cuda-compat.\u001b[0m\n",
      "\u001b[34mDetected cuda-toolkit version: 11.1.\u001b[0m\n",
      "\u001b[34mDetected cuda-compat version: 455.32.00.\u001b[0m\n",
      "\u001b[34mDetected Nvidia driver version: 570.172.08.\u001b[0m\n",
      "\u001b[34mNvidia driver compatible with cuda-toolkit. Disabling cuda-compat.\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:97: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:49 INFO 140213755881280] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/image_classification/default-input.json: {'use_pretrained_model': 0, 'num_layers': 152, 'epochs': 30, 'learning_rate': 0.1, 'lr_scheduler_factor': 0.1, 'optimizer': 'sgd', 'momentum': 0, 'weight_decay': 0.0001, 'beta_1': 0.9, 'beta_2': 0.999, 'eps': 1e-08, 'gamma': 0.9, 'mini_batch_size': 32, 'image_shape': '3,224,224', 'precision_dtype': 'float32'}\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:49 INFO 140213755881280] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'image_shape': '3,32,32', 'num_classes': '2', 'num_training_samples': '5000'}\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:49 INFO 140213755881280] Final configuration: {'use_pretrained_model': 0, 'num_layers': 152, 'epochs': 30, 'learning_rate': 0.1, 'lr_scheduler_factor': 0.1, 'optimizer': 'sgd', 'momentum': 0, 'weight_decay': 0.0001, 'beta_1': 0.9, 'beta_2': 0.999, 'eps': 1e-08, 'gamma': 0.9, 'mini_batch_size': 32, 'image_shape': '3,32,32', 'precision_dtype': 'float32', 'num_classes': '2', 'num_training_samples': '5000'}\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:49 INFO 140213755881280] Searching for .lst files in /opt/ml/input/data/train_lst.\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:49 INFO 140213755881280] Creating record files for train.lst\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:50 INFO 140213755881280] Done creating record files...\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:50 INFO 140213755881280] Searching for .lst files in /opt/ml/input/data/validation_lst.\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:50 INFO 140213755881280] Creating record files for test.lst\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:50 INFO 140213755881280] Done creating record files...\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:50 INFO 140213755881280] use_pretrained_model: 0\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:50 INFO 140213755881280] multi_label: 0\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:50 INFO 140213755881280] Performing random weight initialization\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:50 INFO 140213755881280] ---- Parameters ----\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:50 INFO 140213755881280] num_layers: 152\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:50 INFO 140213755881280] data type: <class 'numpy.float32'>\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:50 INFO 140213755881280] epochs: 30\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:50 INFO 140213755881280] optimizer: sgd\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:50 INFO 140213755881280] momentum: 0.9\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:50 INFO 140213755881280] weight_decay: 0.0001\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:50 INFO 140213755881280] learning_rate: 0.1\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:50 INFO 140213755881280] num_training_samples: 5000\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:50 INFO 140213755881280] mini_batch_size: 32\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:50 INFO 140213755881280] image_shape: 3,32,32\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:50 INFO 140213755881280] num_classes: 2\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:50 INFO 140213755881280] augmentation_type: None\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:50 INFO 140213755881280] kv_store: device\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:50 INFO 140213755881280] checkpoint_frequency not set, will store the best model\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:50 INFO 140213755881280] --------------------\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:42:50 INFO 140213755881280] Setting number of threads: 3\u001b[0m\n",
      "\u001b[34m[20:42:54] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x_ecl_Cuda_11.1.x.441.0/AL2_x86_64/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:43:05 INFO 140213755881280] Epoch[0] Batch [20]#011Speed: 61.572 samples/sec#011accuracy=0.541667\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:43:11 INFO 140213755881280] Epoch[0] Batch [40]#011Speed: 78.592 samples/sec#011accuracy=0.603659\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:43:17 INFO 140213755881280] Epoch[0] Batch [60]#011Speed: 86.377 samples/sec#011accuracy=0.632172\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:43:22 INFO 140213755881280] Epoch[0] Batch [80]#011Speed: 90.937 samples/sec#011accuracy=0.641590\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:43:28 INFO 140213755881280] Epoch[0] Batch [100]#011Speed: 93.900 samples/sec#011accuracy=0.659653\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:43:34 INFO 140213755881280] Epoch[0] Batch [120]#011Speed: 95.998 samples/sec#011accuracy=0.678719\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:43:40 INFO 140213755881280] Epoch[0] Batch [140]#011Speed: 97.503 samples/sec#011accuracy=0.692376\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:43:45 INFO 140213755881280] Epoch[0] Train-accuracy=0.701122\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:43:45 INFO 140213755881280] Epoch[0] Time cost=50.409\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:43:46 INFO 140213755881280] Epoch[0] Validation-accuracy=0.833333\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:43:46 INFO 140213755881280] Storing the best model with validation accuracy: 0.833333\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:43:46 INFO 140213755881280] Saved checkpoint to \"/opt/ml/model/image-classification-0001.params\"\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:43:52 INFO 140213755881280] Epoch[1] Batch [20]#011Speed: 103.603 samples/sec#011accuracy=0.818452\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:43:58 INFO 140213755881280] Epoch[1] Batch [40]#011Speed: 105.478 samples/sec#011accuracy=0.804878\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:44:04 INFO 140213755881280] Epoch[1] Batch [60]#011Speed: 106.060 samples/sec#011accuracy=0.812500\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:44:10 INFO 140213755881280] Epoch[1] Batch [80]#011Speed: 106.397 samples/sec#011accuracy=0.814429\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:44:16 INFO 140213755881280] Epoch[1] Batch [100]#011Speed: 106.543 samples/sec#011accuracy=0.814975\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:44:22 INFO 140213755881280] Epoch[1] Batch [120]#011Speed: 106.607 samples/sec#011accuracy=0.817665\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:44:28 INFO 140213755881280] Epoch[1] Batch [140]#011Speed: 106.654 samples/sec#011accuracy=0.820922\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:44:33 INFO 140213755881280] Epoch[1] Train-accuracy=0.824720\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:44:33 INFO 140213755881280] Epoch[1] Time cost=46.493\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:44:33 INFO 140213755881280] Epoch[1] Validation-accuracy=0.817708\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:44:40 INFO 140213755881280] Epoch[2] Batch [20]#011Speed: 104.506 samples/sec#011accuracy=0.845238\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:44:46 INFO 140213755881280] Epoch[2] Batch [40]#011Speed: 105.694 samples/sec#011accuracy=0.840701\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:44:52 INFO 140213755881280] Epoch[2] Batch [60]#011Speed: 106.099 samples/sec#011accuracy=0.837090\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:44:58 INFO 140213755881280] Epoch[2] Batch [80]#011Speed: 106.272 samples/sec#011accuracy=0.839892\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:45:04 INFO 140213755881280] Epoch[2] Batch [100]#011Speed: 106.319 samples/sec#011accuracy=0.848082\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:45:10 INFO 140213755881280] Epoch[2] Batch [120]#011Speed: 106.425 samples/sec#011accuracy=0.853306\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:45:16 INFO 140213755881280] Epoch[2] Batch [140]#011Speed: 106.486 samples/sec#011accuracy=0.856826\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:45:20 INFO 140213755881280] Epoch[2] Train-accuracy=0.860377\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:45:20 INFO 140213755881280] Epoch[2] Time cost=46.565\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:45:21 INFO 140213755881280] Epoch[2] Validation-accuracy=0.838542\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:45:21 INFO 140213755881280] Storing the best model with validation accuracy: 0.838542\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:45:22 INFO 140213755881280] Saved checkpoint to \"/opt/ml/model/image-classification-0003.params\"\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:45:28 INFO 140213755881280] Epoch[3] Batch [20]#011Speed: 105.045 samples/sec#011accuracy=0.898810\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:45:34 INFO 140213755881280] Epoch[3] Batch [40]#011Speed: 106.026 samples/sec#011accuracy=0.913110\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:45:40 INFO 140213755881280] Epoch[3] Batch [60]#011Speed: 106.282 samples/sec#011accuracy=0.909836\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:45:46 INFO 140213755881280] Epoch[3] Batch [80]#011Speed: 106.384 samples/sec#011accuracy=0.916281\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:45:52 INFO 140213755881280] Epoch[3] Batch [100]#011Speed: 106.395 samples/sec#011accuracy=0.915223\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:45:58 INFO 140213755881280] Epoch[3] Batch [120]#011Speed: 106.380 samples/sec#011accuracy=0.904184\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:46:04 INFO 140213755881280] Epoch[3] Batch [140]#011Speed: 106.405 samples/sec#011accuracy=0.906915\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:46:08 INFO 140213755881280] Epoch[3] Train-accuracy=0.907452\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:46:08 INFO 140213755881280] Epoch[3] Time cost=46.619\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:46:09 INFO 140213755881280] Epoch[3] Validation-accuracy=0.821429\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:46:16 INFO 140213755881280] Epoch[4] Batch [20]#011Speed: 103.624 samples/sec#011accuracy=0.941964\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:46:22 INFO 140213755881280] Epoch[4] Batch [40]#011Speed: 105.028 samples/sec#011accuracy=0.949695\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:46:28 INFO 140213755881280] Epoch[4] Batch [60]#011Speed: 105.451 samples/sec#011accuracy=0.944672\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:46:34 INFO 140213755881280] Epoch[4] Batch [80]#011Speed: 105.718 samples/sec#011accuracy=0.945988\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:46:40 INFO 140213755881280] Epoch[4] Batch [100]#011Speed: 105.865 samples/sec#011accuracy=0.944307\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:46:46 INFO 140213755881280] Epoch[4] Batch [120]#011Speed: 105.950 samples/sec#011accuracy=0.946281\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:46:52 INFO 140213755881280] Epoch[4] Batch [140]#011Speed: 106.042 samples/sec#011accuracy=0.946587\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:46:57 INFO 140213755881280] Epoch[4] Train-accuracy=0.947716\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:46:57 INFO 140213755881280] Epoch[4] Time cost=46.760\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:46:57 INFO 140213755881280] Epoch[4] Validation-accuracy=0.854167\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:46:58 INFO 140213755881280] Storing the best model with validation accuracy: 0.854167\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:46:58 INFO 140213755881280] Saved checkpoint to \"/opt/ml/model/image-classification-0005.params\"\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:47:04 INFO 140213755881280] Epoch[5] Batch [20]#011Speed: 104.405 samples/sec#011accuracy=0.955357\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:47:10 INFO 140213755881280] Epoch[5] Batch [40]#011Speed: 105.460 samples/sec#011accuracy=0.956555\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:47:16 INFO 140213755881280] Epoch[5] Batch [60]#011Speed: 105.765 samples/sec#011accuracy=0.961066\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:47:22 INFO 140213755881280] Epoch[5] Batch [80]#011Speed: 105.963 samples/sec#011accuracy=0.968750\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:47:28 INFO 140213755881280] Epoch[5] Batch [100]#011Speed: 106.088 samples/sec#011accuracy=0.969988\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:47:34 INFO 140213755881280] Epoch[5] Batch [120]#011Speed: 106.135 samples/sec#011accuracy=0.970300\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:47:40 INFO 140213755881280] Epoch[5] Batch [140]#011Speed: 106.181 samples/sec#011accuracy=0.971631\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:47:45 INFO 140213755881280] Epoch[5] Train-accuracy=0.971554\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:47:45 INFO 140213755881280] Epoch[5] Time cost=46.695\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:47:45 INFO 140213755881280] Epoch[5] Validation-accuracy=0.848958\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:47:52 INFO 140213755881280] Epoch[6] Batch [20]#011Speed: 103.659 samples/sec#011accuracy=0.976190\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:47:58 INFO 140213755881280] Epoch[6] Batch [40]#011Speed: 105.067 samples/sec#011accuracy=0.974848\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:48:04 INFO 140213755881280] Epoch[6] Batch [60]#011Speed: 105.561 samples/sec#011accuracy=0.975922\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:48:10 INFO 140213755881280] Epoch[6] Batch [80]#011Speed: 105.811 samples/sec#011accuracy=0.974923\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:48:16 INFO 140213755881280] Epoch[6] Batch [100]#011Speed: 106.004 samples/sec#011accuracy=0.976485\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:48:22 INFO 140213755881280] Epoch[6] Batch [120]#011Speed: 106.108 samples/sec#011accuracy=0.978048\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:48:28 INFO 140213755881280] Epoch[6] Batch [140]#011Speed: 106.147 samples/sec#011accuracy=0.972739\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:48:32 INFO 140213755881280] Epoch[6] Train-accuracy=0.970753\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:48:32 INFO 140213755881280] Epoch[6] Time cost=46.705\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:48:33 INFO 140213755881280] Epoch[6] Validation-accuracy=0.828125\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:48:40 INFO 140213755881280] Epoch[7] Batch [20]#011Speed: 104.224 samples/sec#011accuracy=0.995536\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:48:46 INFO 140213755881280] Epoch[7] Batch [40]#011Speed: 105.495 samples/sec#011accuracy=0.991616\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:48:52 INFO 140213755881280] Epoch[7] Batch [60]#011Speed: 105.801 samples/sec#011accuracy=0.985656\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:48:58 INFO 140213755881280] Epoch[7] Batch [80]#011Speed: 105.991 samples/sec#011accuracy=0.983410\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:49:04 INFO 140213755881280] Epoch[7] Batch [100]#011Speed: 106.118 samples/sec#011accuracy=0.984839\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:49:10 INFO 140213755881280] Epoch[7] Batch [120]#011Speed: 106.161 samples/sec#011accuracy=0.986570\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:49:16 INFO 140213755881280] Epoch[7] Batch [140]#011Speed: 106.238 samples/sec#011accuracy=0.987145\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:49:20 INFO 140213755881280] Epoch[7] Train-accuracy=0.987780\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:49:20 INFO 140213755881280] Epoch[7] Time cost=46.677\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:49:21 INFO 140213755881280] Epoch[7] Validation-accuracy=0.812500\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:49:28 INFO 140213755881280] Epoch[8] Batch [20]#011Speed: 103.296 samples/sec#011accuracy=0.979167\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:49:34 INFO 140213755881280] Epoch[8] Batch [40]#011Speed: 104.998 samples/sec#011accuracy=0.981707\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:49:40 INFO 140213755881280] Epoch[8] Batch [60]#011Speed: 105.512 samples/sec#011accuracy=0.982582\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:49:46 INFO 140213755881280] Epoch[8] Batch [80]#011Speed: 105.833 samples/sec#011accuracy=0.983410\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:49:52 INFO 140213755881280] Epoch[8] Batch [100]#011Speed: 105.988 samples/sec#011accuracy=0.983911\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:49:58 INFO 140213755881280] Epoch[8] Batch [120]#011Speed: 106.102 samples/sec#011accuracy=0.985279\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:50:04 INFO 140213755881280] Epoch[8] Batch [140]#011Speed: 106.113 samples/sec#011accuracy=0.986480\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:50:08 INFO 140213755881280] Epoch[8] Train-accuracy=0.987179\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:50:08 INFO 140213755881280] Epoch[8] Time cost=46.723\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:50:09 INFO 140213755881280] Epoch[8] Validation-accuracy=0.822917\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:50:16 INFO 140213755881280] Epoch[9] Batch [20]#011Speed: 103.546 samples/sec#011accuracy=0.994048\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:50:22 INFO 140213755881280] Epoch[9] Batch [40]#011Speed: 104.754 samples/sec#011accuracy=0.992378\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:50:28 INFO 140213755881280] Epoch[9] Batch [60]#011Speed: 105.387 samples/sec#011accuracy=0.993340\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:50:34 INFO 140213755881280] Epoch[9] Batch [80]#011Speed: 105.680 samples/sec#011accuracy=0.991127\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:50:40 INFO 140213755881280] Epoch[9] Batch [100]#011Speed: 105.880 samples/sec#011accuracy=0.987624\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:50:46 INFO 140213755881280] Epoch[9] Batch [120]#011Speed: 105.978 samples/sec#011accuracy=0.986312\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:50:52 INFO 140213755881280] Epoch[9] Batch [140]#011Speed: 106.053 samples/sec#011accuracy=0.987589\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:50:56 INFO 140213755881280] Epoch[9] Train-accuracy=0.988381\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:50:56 INFO 140213755881280] Epoch[9] Time cost=46.754\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:50:57 INFO 140213755881280] Epoch[9] Validation-accuracy=0.822917\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:51:03 INFO 140213755881280] Epoch[10] Batch [20]#011Speed: 102.918 samples/sec#011accuracy=0.998512\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:51:09 INFO 140213755881280] Epoch[10] Batch [40]#011Speed: 104.682 samples/sec#011accuracy=0.997713\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:51:16 INFO 140213755881280] Epoch[10] Batch [60]#011Speed: 105.284 samples/sec#011accuracy=0.996926\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:51:22 INFO 140213755881280] Epoch[10] Batch [80]#011Speed: 105.543 samples/sec#011accuracy=0.997685\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:51:28 INFO 140213755881280] Epoch[10] Batch [100]#011Speed: 105.715 samples/sec#011accuracy=0.996597\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:51:34 INFO 140213755881280] Epoch[10] Batch [120]#011Speed: 105.844 samples/sec#011accuracy=0.996901\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:51:40 INFO 140213755881280] Epoch[10] Batch [140]#011Speed: 105.906 samples/sec#011accuracy=0.995789\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:51:44 INFO 140213755881280] Epoch[10] Train-accuracy=0.994792\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:51:44 INFO 140213755881280] Epoch[10] Time cost=46.800\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:51:45 INFO 140213755881280] Epoch[10] Validation-accuracy=0.848958\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:51:51 INFO 140213755881280] Epoch[11] Batch [20]#011Speed: 102.446 samples/sec#011accuracy=0.985119\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:51:57 INFO 140213755881280] Epoch[11] Batch [40]#011Speed: 104.268 samples/sec#011accuracy=0.990091\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:52:04 INFO 140213755881280] Epoch[11] Batch [60]#011Speed: 104.913 samples/sec#011accuracy=0.992828\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:52:10 INFO 140213755881280] Epoch[11] Batch [80]#011Speed: 105.208 samples/sec#011accuracy=0.994599\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:52:16 INFO 140213755881280] Epoch[11] Batch [100]#011Speed: 105.440 samples/sec#011accuracy=0.995668\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:52:22 INFO 140213755881280] Epoch[11] Batch [120]#011Speed: 105.575 samples/sec#011accuracy=0.996126\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:52:28 INFO 140213755881280] Epoch[11] Batch [140]#011Speed: 105.622 samples/sec#011accuracy=0.996232\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:52:32 INFO 140213755881280] Epoch[11] Train-accuracy=0.996595\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:52:32 INFO 140213755881280] Epoch[11] Time cost=46.929\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:52:33 INFO 140213755881280] Epoch[11] Validation-accuracy=0.799107\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:52:40 INFO 140213755881280] Epoch[12] Batch [20]#011Speed: 102.679 samples/sec#011accuracy=0.992560\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:52:46 INFO 140213755881280] Epoch[12] Batch [40]#011Speed: 104.643 samples/sec#011accuracy=0.994665\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:52:52 INFO 140213755881280] Epoch[12] Batch [60]#011Speed: 105.423 samples/sec#011accuracy=0.994877\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:52:58 INFO 140213755881280] Epoch[12] Batch [80]#011Speed: 105.817 samples/sec#011accuracy=0.994985\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:53:04 INFO 140213755881280] Epoch[12] Batch [100]#011Speed: 105.992 samples/sec#011accuracy=0.995978\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:53:10 INFO 140213755881280] Epoch[12] Batch [120]#011Speed: 106.163 samples/sec#011accuracy=0.996384\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:53:16 INFO 140213755881280] Epoch[12] Batch [140]#011Speed: 106.299 samples/sec#011accuracy=0.996454\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:53:20 INFO 140213755881280] Epoch[12] Train-accuracy=0.995994\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:53:20 INFO 140213755881280] Epoch[12] Time cost=46.634\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:53:21 INFO 140213755881280] Epoch[12] Validation-accuracy=0.828125\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:53:27 INFO 140213755881280] Epoch[13] Batch [20]#011Speed: 104.841 samples/sec#011accuracy=0.991071\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:53:33 INFO 140213755881280] Epoch[13] Batch [40]#011Speed: 105.933 samples/sec#011accuracy=0.994665\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:53:39 INFO 140213755881280] Epoch[13] Batch [60]#011Speed: 106.336 samples/sec#011accuracy=0.992316\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:53:45 INFO 140213755881280] Epoch[13] Batch [80]#011Speed: 106.500 samples/sec#011accuracy=0.991127\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:53:51 INFO 140213755881280] Epoch[13] Batch [100]#011Speed: 106.629 samples/sec#011accuracy=0.989790\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:53:57 INFO 140213755881280] Epoch[13] Batch [120]#011Speed: 106.673 samples/sec#011accuracy=0.989153\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:54:03 INFO 140213755881280] Epoch[13] Batch [140]#011Speed: 106.652 samples/sec#011accuracy=0.989362\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:54:08 INFO 140213755881280] Epoch[13] Train-accuracy=0.988982\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:54:08 INFO 140213755881280] Epoch[13] Time cost=46.520\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:54:09 INFO 140213755881280] Epoch[13] Validation-accuracy=0.817708\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:54:15 INFO 140213755881280] Epoch[14] Batch [20]#011Speed: 103.173 samples/sec#011accuracy=0.997024\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:54:21 INFO 140213755881280] Epoch[14] Batch [40]#011Speed: 104.620 samples/sec#011accuracy=0.997713\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:54:27 INFO 140213755881280] Epoch[14] Batch [60]#011Speed: 105.086 samples/sec#011accuracy=0.997439\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:54:33 INFO 140213755881280] Epoch[14] Batch [80]#011Speed: 105.362 samples/sec#011accuracy=0.998071\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:54:39 INFO 140213755881280] Epoch[14] Batch [100]#011Speed: 105.530 samples/sec#011accuracy=0.998144\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:54:45 INFO 140213755881280] Epoch[14] Batch [120]#011Speed: 105.572 samples/sec#011accuracy=0.997417\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:54:51 INFO 140213755881280] Epoch[14] Batch [140]#011Speed: 105.655 samples/sec#011accuracy=0.997340\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:54:56 INFO 140213755881280] Epoch[14] Train-accuracy=0.997196\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:54:56 INFO 140213755881280] Epoch[14] Time cost=46.925\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:54:57 INFO 140213755881280] Epoch[14] Validation-accuracy=0.848958\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:55:03 INFO 140213755881280] Epoch[15] Batch [20]#011Speed: 103.260 samples/sec#011accuracy=0.994048\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:55:09 INFO 140213755881280] Epoch[15] Batch [40]#011Speed: 104.759 samples/sec#011accuracy=0.996951\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:55:15 INFO 140213755881280] Epoch[15] Batch [60]#011Speed: 105.078 samples/sec#011accuracy=0.997439\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:55:21 INFO 140213755881280] Epoch[15] Batch [80]#011Speed: 105.352 samples/sec#011accuracy=0.998071\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:55:27 INFO 140213755881280] Epoch[15] Batch [100]#011Speed: 105.541 samples/sec#011accuracy=0.998453\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:55:33 INFO 140213755881280] Epoch[15] Batch [120]#011Speed: 105.659 samples/sec#011accuracy=0.998450\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:55:39 INFO 140213755881280] Epoch[15] Batch [140]#011Speed: 105.750 samples/sec#011accuracy=0.996232\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:55:44 INFO 140213755881280] Epoch[15] Train-accuracy=0.995593\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:55:44 INFO 140213755881280] Epoch[15] Time cost=46.883\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:55:45 INFO 140213755881280] Epoch[15] Validation-accuracy=0.843750\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:55:51 INFO 140213755881280] Epoch[16] Batch [20]#011Speed: 103.298 samples/sec#011accuracy=0.994048\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:55:57 INFO 140213755881280] Epoch[16] Batch [40]#011Speed: 104.765 samples/sec#011accuracy=0.994665\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:56:03 INFO 140213755881280] Epoch[16] Batch [60]#011Speed: 105.203 samples/sec#011accuracy=0.995389\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:56:09 INFO 140213755881280] Epoch[16] Batch [80]#011Speed: 105.471 samples/sec#011accuracy=0.996142\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:56:15 INFO 140213755881280] Epoch[16] Batch [100]#011Speed: 105.581 samples/sec#011accuracy=0.995978\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:56:22 INFO 140213755881280] Epoch[16] Batch [120]#011Speed: 105.338 samples/sec#011accuracy=0.996384\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:56:28 INFO 140213755881280] Epoch[16] Batch [140]#011Speed: 104.969 samples/sec#011accuracy=0.996897\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:56:32 INFO 140213755881280] Epoch[16] Train-accuracy=0.996995\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:56:32 INFO 140213755881280] Epoch[16] Time cost=47.346\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:56:33 INFO 140213755881280] Epoch[16] Validation-accuracy=0.802083\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:56:40 INFO 140213755881280] Epoch[17] Batch [20]#011Speed: 101.080 samples/sec#011accuracy=0.998512\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:56:46 INFO 140213755881280] Epoch[17] Batch [40]#011Speed: 102.561 samples/sec#011accuracy=0.997713\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:56:52 INFO 140213755881280] Epoch[17] Batch [60]#011Speed: 103.254 samples/sec#011accuracy=0.997951\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:56:58 INFO 140213755881280] Epoch[17] Batch [80]#011Speed: 103.610 samples/sec#011accuracy=0.997299\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:57:04 INFO 140213755881280] Epoch[17] Batch [100]#011Speed: 103.783 samples/sec#011accuracy=0.996597\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:57:11 INFO 140213755881280] Epoch[17] Batch [120]#011Speed: 103.865 samples/sec#011accuracy=0.996643\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:57:17 INFO 140213755881280] Epoch[17] Batch [140]#011Speed: 104.065 samples/sec#011accuracy=0.996676\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:57:21 INFO 140213755881280] Epoch[17] Train-accuracy=0.996795\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:57:21 INFO 140213755881280] Epoch[17] Time cost=47.599\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:57:22 INFO 140213755881280] Epoch[17] Validation-accuracy=0.812500\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:57:29 INFO 140213755881280] Epoch[18] Batch [20]#011Speed: 102.841 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:57:35 INFO 140213755881280] Epoch[18] Batch [40]#011Speed: 104.595 samples/sec#011accuracy=0.996189\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:57:41 INFO 140213755881280] Epoch[18] Batch [60]#011Speed: 105.200 samples/sec#011accuracy=0.993852\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:57:47 INFO 140213755881280] Epoch[18] Batch [80]#011Speed: 105.443 samples/sec#011accuracy=0.992284\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:57:53 INFO 140213755881280] Epoch[18] Batch [100]#011Speed: 105.581 samples/sec#011accuracy=0.991955\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:57:59 INFO 140213755881280] Epoch[18] Batch [120]#011Speed: 105.710 samples/sec#011accuracy=0.989928\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:58:05 INFO 140213755881280] Epoch[18] Batch [140]#011Speed: 105.797 samples/sec#011accuracy=0.990913\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:58:09 INFO 140213755881280] Epoch[18] Train-accuracy=0.991587\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:58:09 INFO 140213755881280] Epoch[18] Time cost=46.856\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:58:10 INFO 140213755881280] Epoch[18] Validation-accuracy=0.843750\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:58:17 INFO 140213755881280] Epoch[19] Batch [20]#011Speed: 104.012 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:58:23 INFO 140213755881280] Epoch[19] Batch [40]#011Speed: 105.395 samples/sec#011accuracy=0.999238\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:58:29 INFO 140213755881280] Epoch[19] Batch [60]#011Speed: 105.767 samples/sec#011accuracy=0.998975\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:58:35 INFO 140213755881280] Epoch[19] Batch [80]#011Speed: 105.961 samples/sec#011accuracy=0.999228\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:58:41 INFO 140213755881280] Epoch[19] Batch [100]#011Speed: 106.112 samples/sec#011accuracy=0.999381\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:58:47 INFO 140213755881280] Epoch[19] Batch [120]#011Speed: 106.196 samples/sec#011accuracy=0.998967\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:58:53 INFO 140213755881280] Epoch[19] Batch [140]#011Speed: 106.191 samples/sec#011accuracy=0.998449\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:58:57 INFO 140213755881280] Epoch[19] Train-accuracy=0.997997\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:58:57 INFO 140213755881280] Epoch[19] Time cost=46.696\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:58:58 INFO 140213755881280] Epoch[19] Validation-accuracy=0.875000\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:58:58 INFO 140213755881280] Storing the best model with validation accuracy: 0.875000\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:58:59 INFO 140213755881280] Saved checkpoint to \"/opt/ml/model/image-classification-0020.params\"\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:59:05 INFO 140213755881280] Epoch[20] Batch [20]#011Speed: 103.444 samples/sec#011accuracy=0.995536\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:59:11 INFO 140213755881280] Epoch[20] Batch [40]#011Speed: 104.847 samples/sec#011accuracy=0.996189\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:59:17 INFO 140213755881280] Epoch[20] Batch [60]#011Speed: 105.274 samples/sec#011accuracy=0.995902\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:59:23 INFO 140213755881280] Epoch[20] Batch [80]#011Speed: 105.546 samples/sec#011accuracy=0.996142\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:59:29 INFO 140213755881280] Epoch[20] Batch [100]#011Speed: 105.714 samples/sec#011accuracy=0.996597\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:59:35 INFO 140213755881280] Epoch[20] Batch [120]#011Speed: 105.765 samples/sec#011accuracy=0.996384\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:59:41 INFO 140213755881280] Epoch[20] Batch [140]#011Speed: 105.785 samples/sec#011accuracy=0.996454\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:59:45 INFO 140213755881280] Epoch[20] Train-accuracy=0.996194\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:59:45 INFO 140213755881280] Epoch[20] Time cost=46.862\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:59:46 INFO 140213755881280] Epoch[20] Validation-accuracy=0.875000\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:59:53 INFO 140213755881280] Epoch[21] Batch [20]#011Speed: 103.914 samples/sec#011accuracy=0.989583\u001b[0m\n",
      "\u001b[34m[08/07/2025 20:59:59 INFO 140213755881280] Epoch[21] Batch [40]#011Speed: 105.205 samples/sec#011accuracy=0.987805\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:00:05 INFO 140213755881280] Epoch[21] Batch [60]#011Speed: 105.413 samples/sec#011accuracy=0.988217\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:00:11 INFO 140213755881280] Epoch[21] Batch [80]#011Speed: 105.599 samples/sec#011accuracy=0.989969\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:00:17 INFO 140213755881280] Epoch[21] Batch [100]#011Speed: 105.738 samples/sec#011accuracy=0.991646\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:00:23 INFO 140213755881280] Epoch[21] Batch [120]#011Speed: 105.836 samples/sec#011accuracy=0.992510\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:00:29 INFO 140213755881280] Epoch[21] Batch [140]#011Speed: 105.889 samples/sec#011accuracy=0.993129\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:00:33 INFO 140213755881280] Epoch[21] Train-accuracy=0.993590\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:00:33 INFO 140213755881280] Epoch[21] Time cost=46.817\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:00:34 INFO 140213755881280] Epoch[21] Validation-accuracy=0.838542\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:00:41 INFO 140213755881280] Epoch[22] Batch [20]#011Speed: 103.158 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:00:47 INFO 140213755881280] Epoch[22] Batch [40]#011Speed: 104.754 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:00:53 INFO 140213755881280] Epoch[22] Batch [60]#011Speed: 105.271 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:00:59 INFO 140213755881280] Epoch[22] Batch [80]#011Speed: 105.472 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:01:05 INFO 140213755881280] Epoch[22] Batch [100]#011Speed: 105.613 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:01:11 INFO 140213755881280] Epoch[22] Batch [120]#011Speed: 105.727 samples/sec#011accuracy=0.999742\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:01:17 INFO 140213755881280] Epoch[22] Batch [140]#011Speed: 105.793 samples/sec#011accuracy=0.999778\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:01:21 INFO 140213755881280] Epoch[22] Train-accuracy=0.999599\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:01:21 INFO 140213755881280] Epoch[22] Time cost=46.862\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:01:22 INFO 140213755881280] Epoch[22] Validation-accuracy=0.822917\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:01:29 INFO 140213755881280] Epoch[23] Batch [20]#011Speed: 104.441 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:01:35 INFO 140213755881280] Epoch[23] Batch [40]#011Speed: 105.326 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:01:41 INFO 140213755881280] Epoch[23] Batch [60]#011Speed: 105.652 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:01:47 INFO 140213755881280] Epoch[23] Batch [80]#011Speed: 105.739 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:01:53 INFO 140213755881280] Epoch[23] Batch [100]#011Speed: 105.853 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:01:59 INFO 140213755881280] Epoch[23] Batch [120]#011Speed: 105.907 samples/sec#011accuracy=0.999742\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:02:05 INFO 140213755881280] Epoch[23] Batch [140]#011Speed: 105.950 samples/sec#011accuracy=0.999778\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:02:09 INFO 140213755881280] Epoch[23] Train-accuracy=0.999800\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:02:09 INFO 140213755881280] Epoch[23] Time cost=46.792\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:02:10 INFO 140213755881280] Epoch[23] Validation-accuracy=0.834821\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:02:17 INFO 140213755881280] Epoch[24] Batch [20]#011Speed: 103.923 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:02:23 INFO 140213755881280] Epoch[24] Batch [40]#011Speed: 105.102 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:02:29 INFO 140213755881280] Epoch[24] Batch [60]#011Speed: 105.576 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:02:35 INFO 140213755881280] Epoch[24] Batch [80]#011Speed: 105.737 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:02:41 INFO 140213755881280] Epoch[24] Batch [100]#011Speed: 105.852 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:02:47 INFO 140213755881280] Epoch[24] Batch [120]#011Speed: 105.944 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:02:53 INFO 140213755881280] Epoch[24] Batch [140]#011Speed: 106.005 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:02:58 INFO 140213755881280] Epoch[24] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:02:58 INFO 140213755881280] Epoch[24] Time cost=46.787\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:02:58 INFO 140213755881280] Epoch[24] Validation-accuracy=0.848958\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:03:05 INFO 140213755881280] Epoch[25] Batch [20]#011Speed: 103.289 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:03:11 INFO 140213755881280] Epoch[25] Batch [40]#011Speed: 104.875 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:03:17 INFO 140213755881280] Epoch[25] Batch [60]#011Speed: 105.321 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:03:23 INFO 140213755881280] Epoch[25] Batch [80]#011Speed: 105.549 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:03:29 INFO 140213755881280] Epoch[25] Batch [100]#011Speed: 105.700 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:03:35 INFO 140213755881280] Epoch[25] Batch [120]#011Speed: 105.779 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:03:41 INFO 140213755881280] Epoch[25] Batch [140]#011Speed: 105.837 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:03:46 INFO 140213755881280] Epoch[25] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:03:46 INFO 140213755881280] Epoch[25] Time cost=46.840\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:03:46 INFO 140213755881280] Epoch[25] Validation-accuracy=0.838542\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:03:53 INFO 140213755881280] Epoch[26] Batch [20]#011Speed: 103.649 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:03:59 INFO 140213755881280] Epoch[26] Batch [40]#011Speed: 104.908 samples/sec#011accuracy=0.998476\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:04:05 INFO 140213755881280] Epoch[26] Batch [60]#011Speed: 105.344 samples/sec#011accuracy=0.998975\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:04:11 INFO 140213755881280] Epoch[26] Batch [80]#011Speed: 105.569 samples/sec#011accuracy=0.999228\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:04:17 INFO 140213755881280] Epoch[26] Batch [100]#011Speed: 105.735 samples/sec#011accuracy=0.999381\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:04:23 INFO 140213755881280] Epoch[26] Batch [120]#011Speed: 105.826 samples/sec#011accuracy=0.999483\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:04:29 INFO 140213755881280] Epoch[26] Batch [140]#011Speed: 105.860 samples/sec#011accuracy=0.999557\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:04:34 INFO 140213755881280] Epoch[26] Train-accuracy=0.999599\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:04:34 INFO 140213755881280] Epoch[26] Time cost=46.836\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:04:34 INFO 140213755881280] Epoch[26] Validation-accuracy=0.854167\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:04:41 INFO 140213755881280] Epoch[27] Batch [20]#011Speed: 103.308 samples/sec#011accuracy=0.998512\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:04:47 INFO 140213755881280] Epoch[27] Batch [40]#011Speed: 104.689 samples/sec#011accuracy=0.997713\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:04:53 INFO 140213755881280] Epoch[27] Batch [60]#011Speed: 105.226 samples/sec#011accuracy=0.997951\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:04:59 INFO 140213755881280] Epoch[27] Batch [80]#011Speed: 105.515 samples/sec#011accuracy=0.998071\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:05:05 INFO 140213755881280] Epoch[27] Batch [100]#011Speed: 105.590 samples/sec#011accuracy=0.998453\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:05:11 INFO 140213755881280] Epoch[27] Batch [120]#011Speed: 105.715 samples/sec#011accuracy=0.997934\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:05:17 INFO 140213755881280] Epoch[27] Batch [140]#011Speed: 105.806 samples/sec#011accuracy=0.997784\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:05:22 INFO 140213755881280] Epoch[27] Train-accuracy=0.997596\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:05:22 INFO 140213755881280] Epoch[27] Time cost=46.847\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:05:22 INFO 140213755881280] Epoch[27] Validation-accuracy=0.825893\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:05:29 INFO 140213755881280] Epoch[28] Batch [20]#011Speed: 103.031 samples/sec#011accuracy=0.992560\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:05:35 INFO 140213755881280] Epoch[28] Batch [40]#011Speed: 104.714 samples/sec#011accuracy=0.992378\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:05:41 INFO 140213755881280] Epoch[28] Batch [60]#011Speed: 105.232 samples/sec#011accuracy=0.990779\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:05:47 INFO 140213755881280] Epoch[28] Batch [80]#011Speed: 105.482 samples/sec#011accuracy=0.990355\u001b[0m\n",
      "\u001b[34m[08/07/2025 21:05:53 INFO 140213755881280] Epoch[28] Batch [100]#011Speed: 105.646 samples/sec#011accuracy=0.989171\u001b[0m\n",
      "\n",
      "2025-08-07 21:06:52 Interrupted - Training job interrupted\n",
      "2025-08-07 21:11:08 Starting - Starting the training job\n",
      "2025-08-07 21:11:43 Starting - Insufficient capacity error from EC2 while launching instances, retrying!\n",
      "2025-08-07 21:21:55 Starting - Preparing the instances for training\n",
      "2025-08-07 21:22:27 Downloading - Downloading input data\n",
      "2025-08-07 21:23:33 Downloading - Downloading the training image\n",
      "2025-08-07 21:24:24 Training - Training image download completed. Training in progress.\u001b[32mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[32mRunning default environment configuration script\u001b[0m\n",
      "\u001b[32mNvidia gpu devices, drivers and cuda toolkit versions (only available on hosts with GPU):\u001b[0m\n",
      "\u001b[32mThu Aug  7 21:24:37 2025       \u001b[0m\n",
      "\u001b[32m+-----------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[32m| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\u001b[0m\n",
      "\u001b[32m|-----------------------------------------+------------------------+----------------------+\u001b[0m\n",
      "\u001b[32m| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\u001b[0m\n",
      "\u001b[32m| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\u001b[0m\n",
      "\u001b[32m|                                         |                        |               MIG M. |\u001b[0m\n",
      "\u001b[32m|=========================================+========================+======================|\u001b[0m\n",
      "\u001b[32m|   0  Tesla T4                       On  |   00000000:00:1E.0 Off |                    0 |\u001b[0m\n",
      "\u001b[32m| N/A   32C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\u001b[0m\n",
      "\u001b[32m|                                         |                        |                  N/A |\u001b[0m\n",
      "\u001b[32m+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \u001b[0m\n",
      "\u001b[32m+-----------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[32m| Processes:                                                                              |\u001b[0m\n",
      "\u001b[32m|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\u001b[0m\n",
      "\u001b[32m|        ID   ID                                                               Usage      |\u001b[0m\n",
      "\u001b[32m|=========================================================================================|\u001b[0m\n",
      "\u001b[32m|  No running processes found                                                             |\u001b[0m\n",
      "\u001b[32m+-----------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[32mChecking for nvidia driver and cuda compatibility.\u001b[0m\n",
      "\u001b[32mCUDA Compatibility driver provided.\u001b[0m\n",
      "\u001b[32mProceeding with compatibility check between driver, cuda-toolkit and cuda-compat.\u001b[0m\n",
      "\u001b[32mDetected cuda-toolkit version: 11.1.\u001b[0m\n",
      "\u001b[32mDetected cuda-compat version: 455.32.00.\u001b[0m\n",
      "\u001b[32mDetected Nvidia driver version: 570.172.08.\u001b[0m\n",
      "\u001b[32mNvidia driver compatible with cuda-toolkit. Disabling cuda-compat.\u001b[0m\n",
      "\u001b[32m/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:97: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/image_classification/default-input.json: {'use_pretrained_model': 0, 'num_layers': 152, 'epochs': 30, 'learning_rate': 0.1, 'lr_scheduler_factor': 0.1, 'optimizer': 'sgd', 'momentum': 0, 'weight_decay': 0.0001, 'beta_1': 0.9, 'beta_2': 0.999, 'eps': 1e-08, 'gamma': 0.9, 'mini_batch_size': 32, 'image_shape': '3,224,224', 'precision_dtype': 'float32'}\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'image_shape': '3,32,32', 'num_classes': '2', 'num_training_samples': '5000'}\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] Final configuration: {'use_pretrained_model': 0, 'num_layers': 152, 'epochs': 30, 'learning_rate': 0.1, 'lr_scheduler_factor': 0.1, 'optimizer': 'sgd', 'momentum': 0, 'weight_decay': 0.0001, 'beta_1': 0.9, 'beta_2': 0.999, 'eps': 1e-08, 'gamma': 0.9, 'mini_batch_size': 32, 'image_shape': '3,32,32', 'precision_dtype': 'float32', 'num_classes': '2', 'num_training_samples': '5000'}\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] Searching for .lst files in /opt/ml/input/data/train_lst.\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] Creating record files for train.lst\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] Done creating record files...\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] Searching for .lst files in /opt/ml/input/data/validation_lst.\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] Creating record files for test.lst\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] Done creating record files...\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] use_pretrained_model: 0\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] multi_label: 0\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] Performing random weight initialization\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] ---- Parameters ----\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] num_layers: 152\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] data type: <class 'numpy.float32'>\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] epochs: 30\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] optimizer: sgd\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] momentum: 0.9\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] weight_decay: 0.0001\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] learning_rate: 0.1\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] num_training_samples: 5000\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] mini_batch_size: 32\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] image_shape: 3,32,32\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] num_classes: 2\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] augmentation_type: None\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] kv_store: device\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] checkpoint_frequency not set, will store the best model\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] --------------------\u001b[0m\n",
      "\u001b[32m/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:40 INFO 140248386783040] Setting number of threads: 3\u001b[0m\n",
      "\u001b[32m[21:24:46] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x_ecl_Cuda_11.1.x.441.0/AL2_x86_64/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:24:56 INFO 140248386783040] Epoch[0] Batch [20]#011Speed: 60.800 samples/sec#011accuracy=0.584821\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:25:03 INFO 140248386783040] Epoch[0] Batch [40]#011Speed: 75.556 samples/sec#011accuracy=0.620427\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:25:09 INFO 140248386783040] Epoch[0] Batch [60]#011Speed: 82.521 samples/sec#011accuracy=0.642418\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:25:15 INFO 140248386783040] Epoch[0] Batch [80]#011Speed: 85.989 samples/sec#011accuracy=0.641590\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:25:22 INFO 140248386783040] Epoch[0] Batch [100]#011Speed: 89.172 samples/sec#011accuracy=0.645730\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:25:28 INFO 140248386783040] Epoch[0] Batch [120]#011Speed: 91.540 samples/sec#011accuracy=0.666581\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:25:34 INFO 140248386783040] Epoch[0] Batch [140]#011Speed: 93.299 samples/sec#011accuracy=0.683954\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:25:38 INFO 140248386783040] Epoch[0] Train-accuracy=0.694912\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:25:38 INFO 140248386783040] Epoch[0] Time cost=52.577\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:25:39 INFO 140248386783040] Epoch[0] Validation-accuracy=0.812500\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:25:39 INFO 140248386783040] Storing the best model with validation accuracy: 0.812500\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:25:40 INFO 140248386783040] Saved checkpoint to \"/opt/ml/model/image-classification-0001.params\"\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:25:46 INFO 140248386783040] Epoch[1] Batch [20]#011Speed: 103.137 samples/sec#011accuracy=0.788690\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:25:52 INFO 140248386783040] Epoch[1] Batch [40]#011Speed: 104.390 samples/sec#011accuracy=0.779726\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:25:58 INFO 140248386783040] Epoch[1] Batch [60]#011Speed: 104.953 samples/sec#011accuracy=0.794057\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:26:04 INFO 140248386783040] Epoch[1] Batch [80]#011Speed: 105.241 samples/sec#011accuracy=0.791667\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:26:10 INFO 140248386783040] Epoch[1] Batch [100]#011Speed: 105.496 samples/sec#011accuracy=0.793317\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:26:16 INFO 140248386783040] Epoch[1] Batch [120]#011Speed: 105.636 samples/sec#011accuracy=0.801136\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:26:22 INFO 140248386783040] Epoch[1] Batch [140]#011Speed: 105.703 samples/sec#011accuracy=0.804300\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:26:27 INFO 140248386783040] Epoch[1] Train-accuracy=0.805689\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:26:27 INFO 140248386783040] Epoch[1] Time cost=46.911\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:26:27 INFO 140248386783040] Epoch[1] Validation-accuracy=0.833333\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:26:28 INFO 140248386783040] Storing the best model with validation accuracy: 0.833333\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:26:28 INFO 140248386783040] Saved checkpoint to \"/opt/ml/model/image-classification-0002.params\"\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:26:34 INFO 140248386783040] Epoch[2] Batch [20]#011Speed: 104.061 samples/sec#011accuracy=0.843750\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:26:40 INFO 140248386783040] Epoch[2] Batch [40]#011Speed: 104.901 samples/sec#011accuracy=0.845274\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:26:46 INFO 140248386783040] Epoch[2] Batch [60]#011Speed: 104.786 samples/sec#011accuracy=0.844775\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:26:53 INFO 140248386783040] Epoch[2] Batch [80]#011Speed: 104.695 samples/sec#011accuracy=0.847994\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:26:59 INFO 140248386783040] Epoch[2] Batch [100]#011Speed: 104.450 samples/sec#011accuracy=0.859839\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:27:05 INFO 140248386783040] Epoch[2] Batch [120]#011Speed: 104.370 samples/sec#011accuracy=0.860021\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:27:11 INFO 140248386783040] Epoch[2] Batch [140]#011Speed: 104.263 samples/sec#011accuracy=0.864805\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:27:16 INFO 140248386783040] Epoch[2] Train-accuracy=0.868189\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:27:16 INFO 140248386783040] Epoch[2] Time cost=47.510\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:27:16 INFO 140248386783040] Epoch[2] Validation-accuracy=0.807292\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:27:23 INFO 140248386783040] Epoch[3] Batch [20]#011Speed: 102.138 samples/sec#011accuracy=0.910714\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:27:29 INFO 140248386783040] Epoch[3] Batch [40]#011Speed: 104.134 samples/sec#011accuracy=0.917683\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:27:35 INFO 140248386783040] Epoch[3] Batch [60]#011Speed: 104.670 samples/sec#011accuracy=0.906250\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:27:41 INFO 140248386783040] Epoch[3] Batch [80]#011Speed: 104.257 samples/sec#011accuracy=0.905864\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:27:47 INFO 140248386783040] Epoch[3] Batch [100]#011Speed: 104.450 samples/sec#011accuracy=0.901609\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:27:53 INFO 140248386783040] Epoch[3] Batch [120]#011Speed: 104.646 samples/sec#011accuracy=0.903151\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:28:00 INFO 140248386783040] Epoch[3] Batch [140]#011Speed: 104.794 samples/sec#011accuracy=0.912234\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:28:04 INFO 140248386783040] Epoch[3] Train-accuracy=0.913261\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:28:04 INFO 140248386783040] Epoch[3] Time cost=47.286\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:28:05 INFO 140248386783040] Epoch[3] Validation-accuracy=0.830357\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:28:12 INFO 140248386783040] Epoch[4] Batch [20]#011Speed: 101.864 samples/sec#011accuracy=0.913690\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:28:18 INFO 140248386783040] Epoch[4] Batch [40]#011Speed: 103.573 samples/sec#011accuracy=0.926829\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:28:24 INFO 140248386783040] Epoch[4] Batch [60]#011Speed: 104.126 samples/sec#011accuracy=0.940574\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:28:30 INFO 140248386783040] Epoch[4] Batch [80]#011Speed: 104.597 samples/sec#011accuracy=0.949074\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:28:36 INFO 140248386783040] Epoch[4] Batch [100]#011Speed: 104.785 samples/sec#011accuracy=0.944307\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:28:42 INFO 140248386783040] Epoch[4] Batch [120]#011Speed: 104.980 samples/sec#011accuracy=0.942665\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:28:48 INFO 140248386783040] Epoch[4] Batch [140]#011Speed: 105.117 samples/sec#011accuracy=0.947473\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:28:52 INFO 140248386783040] Epoch[4] Train-accuracy=0.949119\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:28:52 INFO 140248386783040] Epoch[4] Time cost=47.152\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:28:53 INFO 140248386783040] Epoch[4] Validation-accuracy=0.843750\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:28:53 INFO 140248386783040] Storing the best model with validation accuracy: 0.843750\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:28:54 INFO 140248386783040] Saved checkpoint to \"/opt/ml/model/image-classification-0005.params\"\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:29:00 INFO 140248386783040] Epoch[5] Batch [20]#011Speed: 103.706 samples/sec#011accuracy=0.933036\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:29:06 INFO 140248386783040] Epoch[5] Batch [40]#011Speed: 104.158 samples/sec#011accuracy=0.928354\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:29:12 INFO 140248386783040] Epoch[5] Batch [60]#011Speed: 103.870 samples/sec#011accuracy=0.936475\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:29:18 INFO 140248386783040] Epoch[5] Batch [80]#011Speed: 104.323 samples/sec#011accuracy=0.945216\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:29:24 INFO 140248386783040] Epoch[5] Batch [100]#011Speed: 104.661 samples/sec#011accuracy=0.950186\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:29:30 INFO 140248386783040] Epoch[5] Batch [120]#011Speed: 104.921 samples/sec#011accuracy=0.954287\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:29:36 INFO 140248386783040] Epoch[5] Batch [140]#011Speed: 105.085 samples/sec#011accuracy=0.959885\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:29:41 INFO 140248386783040] Epoch[5] Train-accuracy=0.962740\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:29:41 INFO 140248386783040] Epoch[5] Time cost=47.175\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:29:42 INFO 140248386783040] Epoch[5] Validation-accuracy=0.765625\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:29:48 INFO 140248386783040] Epoch[6] Batch [20]#011Speed: 101.519 samples/sec#011accuracy=0.995536\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:29:54 INFO 140248386783040] Epoch[6] Batch [40]#011Speed: 103.733 samples/sec#011accuracy=0.992378\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:30:01 INFO 140248386783040] Epoch[6] Batch [60]#011Speed: 104.524 samples/sec#011accuracy=0.991291\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:30:07 INFO 140248386783040] Epoch[6] Batch [80]#011Speed: 104.663 samples/sec#011accuracy=0.985340\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:30:13 INFO 140248386783040] Epoch[6] Batch [100]#011Speed: 104.879 samples/sec#011accuracy=0.982054\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:30:19 INFO 140248386783040] Epoch[6] Batch [120]#011Speed: 105.037 samples/sec#011accuracy=0.979597\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:30:25 INFO 140248386783040] Epoch[6] Batch [140]#011Speed: 105.143 samples/sec#011accuracy=0.979832\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:30:29 INFO 140248386783040] Epoch[6] Train-accuracy=0.980569\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:30:29 INFO 140248386783040] Epoch[6] Time cost=47.144\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:30:30 INFO 140248386783040] Epoch[6] Validation-accuracy=0.817708\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:30:37 INFO 140248386783040] Epoch[7] Batch [20]#011Speed: 101.607 samples/sec#011accuracy=0.995536\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:30:43 INFO 140248386783040] Epoch[7] Batch [40]#011Speed: 103.202 samples/sec#011accuracy=0.992378\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:30:49 INFO 140248386783040] Epoch[7] Batch [60]#011Speed: 103.961 samples/sec#011accuracy=0.989242\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:30:55 INFO 140248386783040] Epoch[7] Batch [80]#011Speed: 104.388 samples/sec#011accuracy=0.990355\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:31:01 INFO 140248386783040] Epoch[7] Batch [100]#011Speed: 104.725 samples/sec#011accuracy=0.987005\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:31:07 INFO 140248386783040] Epoch[7] Batch [120]#011Speed: 104.922 samples/sec#011accuracy=0.987345\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:31:13 INFO 140248386783040] Epoch[7] Batch [140]#011Speed: 105.059 samples/sec#011accuracy=0.988475\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:31:18 INFO 140248386783040] Epoch[7] Train-accuracy=0.989583\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:31:18 INFO 140248386783040] Epoch[7] Time cost=47.159\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:31:19 INFO 140248386783040] Epoch[7] Validation-accuracy=0.816964\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:31:25 INFO 140248386783040] Epoch[8] Batch [20]#011Speed: 101.942 samples/sec#011accuracy=0.994048\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:31:31 INFO 140248386783040] Epoch[8] Batch [40]#011Speed: 104.016 samples/sec#011accuracy=0.991616\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:31:37 INFO 140248386783040] Epoch[8] Batch [60]#011Speed: 104.657 samples/sec#011accuracy=0.992316\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:31:43 INFO 140248386783040] Epoch[8] Batch [80]#011Speed: 104.884 samples/sec#011accuracy=0.991512\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:31:49 INFO 140248386783040] Epoch[8] Batch [100]#011Speed: 104.827 samples/sec#011accuracy=0.991955\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:31:55 INFO 140248386783040] Epoch[8] Batch [120]#011Speed: 104.906 samples/sec#011accuracy=0.992510\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:32:02 INFO 140248386783040] Epoch[8] Batch [140]#011Speed: 104.907 samples/sec#011accuracy=0.992908\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:32:06 INFO 140248386783040] Epoch[8] Train-accuracy=0.993389\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:32:06 INFO 140248386783040] Epoch[8] Time cost=47.211\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:32:07 INFO 140248386783040] Epoch[8] Validation-accuracy=0.796875\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:32:13 INFO 140248386783040] Epoch[9] Batch [20]#011Speed: 102.508 samples/sec#011accuracy=0.997024\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:32:20 INFO 140248386783040] Epoch[9] Batch [40]#011Speed: 104.363 samples/sec#011accuracy=0.995427\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:32:26 INFO 140248386783040] Epoch[9] Batch [60]#011Speed: 105.082 samples/sec#011accuracy=0.995389\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:32:32 INFO 140248386783040] Epoch[9] Batch [80]#011Speed: 105.211 samples/sec#011accuracy=0.994599\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:32:38 INFO 140248386783040] Epoch[9] Batch [100]#011Speed: 105.442 samples/sec#011accuracy=0.994740\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:32:44 INFO 140248386783040] Epoch[9] Batch [120]#011Speed: 105.604 samples/sec#011accuracy=0.994318\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:32:50 INFO 140248386783040] Epoch[9] Batch [140]#011Speed: 105.647 samples/sec#011accuracy=0.995124\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:32:54 INFO 140248386783040] Epoch[9] Train-accuracy=0.994591\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:32:54 INFO 140248386783040] Epoch[9] Time cost=46.953\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:32:55 INFO 140248386783040] Epoch[9] Validation-accuracy=0.796875\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:33:02 INFO 140248386783040] Epoch[10] Batch [20]#011Speed: 102.215 samples/sec#011accuracy=0.992560\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:33:08 INFO 140248386783040] Epoch[10] Batch [40]#011Speed: 104.041 samples/sec#011accuracy=0.990854\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:33:14 INFO 140248386783040] Epoch[10] Batch [60]#011Speed: 104.493 samples/sec#011accuracy=0.993340\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:33:20 INFO 140248386783040] Epoch[10] Batch [80]#011Speed: 104.930 samples/sec#011accuracy=0.993827\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:33:26 INFO 140248386783040] Epoch[10] Batch [100]#011Speed: 105.181 samples/sec#011accuracy=0.993502\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:33:32 INFO 140248386783040] Epoch[10] Batch [120]#011Speed: 105.165 samples/sec#011accuracy=0.992252\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:33:38 INFO 140248386783040] Epoch[10] Batch [140]#011Speed: 105.227 samples/sec#011accuracy=0.990248\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:33:42 INFO 140248386783040] Epoch[10] Train-accuracy=0.986779\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:33:42 INFO 140248386783040] Epoch[10] Time cost=47.110\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:33:43 INFO 140248386783040] Epoch[10] Validation-accuracy=0.776042\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:33:50 INFO 140248386783040] Epoch[11] Batch [20]#011Speed: 101.528 samples/sec#011accuracy=0.973214\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:33:56 INFO 140248386783040] Epoch[11] Batch [40]#011Speed: 103.222 samples/sec#011accuracy=0.986280\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:34:02 INFO 140248386783040] Epoch[11] Batch [60]#011Speed: 103.953 samples/sec#011accuracy=0.990266\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:34:08 INFO 140248386783040] Epoch[11] Batch [80]#011Speed: 104.424 samples/sec#011accuracy=0.992670\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:34:14 INFO 140248386783040] Epoch[11] Batch [100]#011Speed: 104.662 samples/sec#011accuracy=0.994121\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:34:20 INFO 140248386783040] Epoch[11] Batch [120]#011Speed: 104.543 samples/sec#011accuracy=0.994835\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:34:27 INFO 140248386783040] Epoch[11] Batch [140]#011Speed: 104.370 samples/sec#011accuracy=0.995567\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:34:31 INFO 140248386783040] Epoch[11] Train-accuracy=0.995793\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:34:31 INFO 140248386783040] Epoch[11] Time cost=47.525\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:34:32 INFO 140248386783040] Epoch[11] Validation-accuracy=0.785714\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:34:39 INFO 140248386783040] Epoch[12] Batch [20]#011Speed: 99.863 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:34:45 INFO 140248386783040] Epoch[12] Batch [40]#011Speed: 102.167 samples/sec#011accuracy=0.999238\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:34:51 INFO 140248386783040] Epoch[12] Batch [60]#011Speed: 102.739 samples/sec#011accuracy=0.999488\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:34:57 INFO 140248386783040] Epoch[12] Batch [80]#011Speed: 102.854 samples/sec#011accuracy=0.999614\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:35:04 INFO 140248386783040] Epoch[12] Batch [100]#011Speed: 102.736 samples/sec#011accuracy=0.999691\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:35:10 INFO 140248386783040] Epoch[12] Batch [120]#011Speed: 102.996 samples/sec#011accuracy=0.999742\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:35:16 INFO 140248386783040] Epoch[12] Batch [140]#011Speed: 102.828 samples/sec#011accuracy=0.999778\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:35:21 INFO 140248386783040] Epoch[12] Train-accuracy=0.999800\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:35:21 INFO 140248386783040] Epoch[12] Time cost=48.172\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:35:21 INFO 140248386783040] Epoch[12] Validation-accuracy=0.791667\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:35:28 INFO 140248386783040] Epoch[13] Batch [20]#011Speed: 101.059 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:35:34 INFO 140248386783040] Epoch[13] Batch [40]#011Speed: 102.828 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:35:40 INFO 140248386783040] Epoch[13] Batch [60]#011Speed: 103.431 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:35:46 INFO 140248386783040] Epoch[13] Batch [80]#011Speed: 103.885 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:35:52 INFO 140248386783040] Epoch[13] Batch [100]#011Speed: 104.243 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:35:59 INFO 140248386783040] Epoch[13] Batch [120]#011Speed: 104.409 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:36:05 INFO 140248386783040] Epoch[13] Batch [140]#011Speed: 104.483 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:36:09 INFO 140248386783040] Epoch[13] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:36:09 INFO 140248386783040] Epoch[13] Time cost=47.427\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:36:10 INFO 140248386783040] Epoch[13] Validation-accuracy=0.796875\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:36:17 INFO 140248386783040] Epoch[14] Batch [20]#011Speed: 101.742 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:36:23 INFO 140248386783040] Epoch[14] Batch [40]#011Speed: 103.601 samples/sec#011accuracy=0.999238\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:36:29 INFO 140248386783040] Epoch[14] Batch [60]#011Speed: 104.209 samples/sec#011accuracy=0.999488\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:36:35 INFO 140248386783040] Epoch[14] Batch [80]#011Speed: 104.701 samples/sec#011accuracy=0.999614\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:36:41 INFO 140248386783040] Epoch[14] Batch [100]#011Speed: 105.076 samples/sec#011accuracy=0.999691\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:36:47 INFO 140248386783040] Epoch[14] Batch [120]#011Speed: 105.290 samples/sec#011accuracy=0.999483\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:36:53 INFO 140248386783040] Epoch[14] Batch [140]#011Speed: 105.370 samples/sec#011accuracy=0.999557\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:36:57 INFO 140248386783040] Epoch[14] Train-accuracy=0.999599\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:36:57 INFO 140248386783040] Epoch[14] Time cost=47.055\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:36:58 INFO 140248386783040] Epoch[14] Validation-accuracy=0.796875\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:37:05 INFO 140248386783040] Epoch[15] Batch [20]#011Speed: 101.017 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:37:11 INFO 140248386783040] Epoch[15] Batch [40]#011Speed: 103.609 samples/sec#011accuracy=0.999238\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:37:17 INFO 140248386783040] Epoch[15] Batch [60]#011Speed: 104.640 samples/sec#011accuracy=0.998975\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:37:23 INFO 140248386783040] Epoch[15] Batch [80]#011Speed: 105.023 samples/sec#011accuracy=0.999228\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:37:29 INFO 140248386783040] Epoch[15] Batch [100]#011Speed: 105.002 samples/sec#011accuracy=0.999072\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:37:35 INFO 140248386783040] Epoch[15] Batch [120]#011Speed: 105.074 samples/sec#011accuracy=0.998967\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:37:41 INFO 140248386783040] Epoch[15] Batch [140]#011Speed: 105.273 samples/sec#011accuracy=0.998892\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:37:46 INFO 140248386783040] Epoch[15] Train-accuracy=0.998798\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:37:46 INFO 140248386783040] Epoch[15] Time cost=47.089\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:37:47 INFO 140248386783040] Epoch[15] Validation-accuracy=0.794643\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:37:53 INFO 140248386783040] Epoch[16] Batch [20]#011Speed: 103.313 samples/sec#011accuracy=0.997024\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:37:59 INFO 140248386783040] Epoch[16] Batch [40]#011Speed: 104.033 samples/sec#011accuracy=0.995427\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:38:05 INFO 140248386783040] Epoch[16] Batch [60]#011Speed: 104.869 samples/sec#011accuracy=0.993340\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:38:11 INFO 140248386783040] Epoch[16] Batch [80]#011Speed: 105.224 samples/sec#011accuracy=0.990741\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:38:17 INFO 140248386783040] Epoch[16] Batch [100]#011Speed: 105.310 samples/sec#011accuracy=0.989790\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:38:24 INFO 140248386783040] Epoch[16] Batch [120]#011Speed: 105.131 samples/sec#011accuracy=0.990702\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:38:30 INFO 140248386783040] Epoch[16] Batch [140]#011Speed: 105.265 samples/sec#011accuracy=0.991578\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:38:34 INFO 140248386783040] Epoch[16] Train-accuracy=0.991987\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:38:34 INFO 140248386783040] Epoch[16] Time cost=47.078\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:38:35 INFO 140248386783040] Epoch[16] Validation-accuracy=0.833333\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:38:41 INFO 140248386783040] Epoch[17] Batch [20]#011Speed: 103.092 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:38:48 INFO 140248386783040] Epoch[17] Batch [40]#011Speed: 104.364 samples/sec#011accuracy=0.998476\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:38:54 INFO 140248386783040] Epoch[17] Batch [60]#011Speed: 104.960 samples/sec#011accuracy=0.998975\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:39:00 INFO 140248386783040] Epoch[17] Batch [80]#011Speed: 105.129 samples/sec#011accuracy=0.998457\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:39:06 INFO 140248386783040] Epoch[17] Batch [100]#011Speed: 105.388 samples/sec#011accuracy=0.998144\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:39:12 INFO 140248386783040] Epoch[17] Batch [120]#011Speed: 105.573 samples/sec#011accuracy=0.998192\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:39:18 INFO 140248386783040] Epoch[17] Batch [140]#011Speed: 105.729 samples/sec#011accuracy=0.998227\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:39:22 INFO 140248386783040] Epoch[17] Train-accuracy=0.997596\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:39:22 INFO 140248386783040] Epoch[17] Time cost=46.908\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:39:23 INFO 140248386783040] Epoch[17] Validation-accuracy=0.822917\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:39:29 INFO 140248386783040] Epoch[18] Batch [20]#011Speed: 103.494 samples/sec#011accuracy=0.997024\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:39:36 INFO 140248386783040] Epoch[18] Batch [40]#011Speed: 104.716 samples/sec#011accuracy=0.993902\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:39:42 INFO 140248386783040] Epoch[18] Batch [60]#011Speed: 105.164 samples/sec#011accuracy=0.993852\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:39:48 INFO 140248386783040] Epoch[18] Batch [80]#011Speed: 105.497 samples/sec#011accuracy=0.993441\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:39:54 INFO 140248386783040] Epoch[18] Batch [100]#011Speed: 105.714 samples/sec#011accuracy=0.994121\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:40:00 INFO 140248386783040] Epoch[18] Batch [120]#011Speed: 105.856 samples/sec#011accuracy=0.993285\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:40:06 INFO 140248386783040] Epoch[18] Batch [140]#011Speed: 105.790 samples/sec#011accuracy=0.992686\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:40:10 INFO 140248386783040] Epoch[18] Train-accuracy=0.992989\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:40:10 INFO 140248386783040] Epoch[18] Time cost=46.865\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:40:11 INFO 140248386783040] Epoch[18] Validation-accuracy=0.781250\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:40:18 INFO 140248386783040] Epoch[19] Batch [20]#011Speed: 104.433 samples/sec#011accuracy=0.997024\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:40:24 INFO 140248386783040] Epoch[19] Batch [40]#011Speed: 104.857 samples/sec#011accuracy=0.996189\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:40:30 INFO 140248386783040] Epoch[19] Batch [60]#011Speed: 105.458 samples/sec#011accuracy=0.997439\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:40:36 INFO 140248386783040] Epoch[19] Batch [80]#011Speed: 105.652 samples/sec#011accuracy=0.998071\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:40:42 INFO 140248386783040] Epoch[19] Batch [100]#011Speed: 105.751 samples/sec#011accuracy=0.998453\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:40:48 INFO 140248386783040] Epoch[19] Batch [120]#011Speed: 105.819 samples/sec#011accuracy=0.998709\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:40:54 INFO 140248386783040] Epoch[19] Batch [140]#011Speed: 105.936 samples/sec#011accuracy=0.998227\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:40:58 INFO 140248386783040] Epoch[19] Train-accuracy=0.998397\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:40:58 INFO 140248386783040] Epoch[19] Time cost=46.780\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:40:59 INFO 140248386783040] Epoch[19] Validation-accuracy=0.794643\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:41:06 INFO 140248386783040] Epoch[20] Batch [20]#011Speed: 103.771 samples/sec#011accuracy=0.998512\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:41:12 INFO 140248386783040] Epoch[20] Batch [40]#011Speed: 105.132 samples/sec#011accuracy=0.998476\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:41:18 INFO 140248386783040] Epoch[20] Batch [60]#011Speed: 105.457 samples/sec#011accuracy=0.998463\u001b[0m\n",
      "\n",
      "2025-08-07 21:41:27 Stopping - Stopping the training job\u001b[32m[08/07/2025 21:41:24 INFO 140248386783040] Epoch[20] Batch [80]#011Speed: 105.571 samples/sec#011accuracy=0.998457\u001b[0m\n",
      "\u001b[32mException ignored on calling ctypes callback function: <function _updater_wrapper.<locals>.updater_handle at 0x7f8d96b74b80>\u001b[0m\n",
      "\u001b[32mTraceback (most recent call last):\n",
      "  File \"/opt/amazon/lib/python3.8/site-packages/mxnet/kvstore.py\", line 84, in updater_handle\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:41:28 INFO 140248386783040] Training stopped.\n",
      "    updater(key, lhs, rhs)\n",
      "  File \"/opt/amazon/lib/python3.8/site-packages/mxnet/optimizer/optimizer.py\", line 1530, in __call__\n",
      "    self.optimizer.update_multi_precision(index, weight, grad, self.states[index])\n",
      "  File \"/opt/amazon/lib/python3.8/site-packages/mxnet/optimizer/optimizer.py\", line 554, in update_multi_precision\n",
      "    self._update_impl(index, weight, grad, state,\n",
      "  File \"/opt/amazon/lib/python3.8/site-packages/mxnet/optimizer/optimizer.py\", line 536, in _update_impl\n",
      "    sgd_mom_update(weight, grad, state, out=weight,\n",
      "  File \"<string>\", line 95, in sgd_mom_update\n",
      "  File \"/opt/amazon/lib/python3.8/site-packages/mxnet/_ctypes/ndarray.py\", line 90, in _imperative_invoke\n",
      "    c_str_array(keys),\n",
      "  File \"/opt/amazon/lib/python3.8/site-packages/mxnet/base.py\", line 330, in c_str_array\n",
      "    arr[:] = [s.encode('utf-8') for s in strings]\n",
      "  File \"/opt/amazon/lib/python3.8/site-packages/image_classification/start.py\", line 121, in exit_gracefully\n",
      "    sys.exit(0)\u001b[0m\n",
      "\u001b[32mSystemExit: 0\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:41:30 INFO 140248386783040] Epoch[20] Batch [100]#011Speed: 105.141 samples/sec#011accuracy=0.998453\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:41:36 INFO 140248386783040] Epoch[20] Batch [120]#011Speed: 104.947 samples/sec#011accuracy=0.998192\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:41:42 INFO 140248386783040] Epoch[20] Batch [140]#011Speed: 104.804 samples/sec#011accuracy=0.998227\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:41:47 INFO 140248386783040] Epoch[20] Train-accuracy=0.997796\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:41:47 INFO 140248386783040] Epoch[20] Time cost=47.335\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:41:48 INFO 140248386783040] Epoch[20] Validation-accuracy=0.817708\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:41:54 INFO 140248386783040] Epoch[21] Batch [20]#011Speed: 103.441 samples/sec#011accuracy=0.998512\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:42:00 INFO 140248386783040] Epoch[21] Batch [40]#011Speed: 104.274 samples/sec#011accuracy=0.993140\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:42:06 INFO 140248386783040] Epoch[21] Batch [60]#011Speed: 104.506 samples/sec#011accuracy=0.990779\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:42:12 INFO 140248386783040] Epoch[21] Batch [80]#011Speed: 104.626 samples/sec#011accuracy=0.989198\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:42:19 INFO 140248386783040] Epoch[21] Batch [100]#011Speed: 104.456 samples/sec#011accuracy=0.990099\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:42:25 INFO 140248386783040] Epoch[21] Batch [120]#011Speed: 104.556 samples/sec#011accuracy=0.989669\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:42:31 INFO 140248386783040] Epoch[21] Batch [140]#011Speed: 104.495 samples/sec#011accuracy=0.990248\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:42:35 INFO 140248386783040] Epoch[21] Train-accuracy=0.990785\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:42:35 INFO 140248386783040] Epoch[21] Time cost=47.514\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:42:36 INFO 140248386783040] Epoch[21] Validation-accuracy=0.817708\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:42:43 INFO 140248386783040] Epoch[22] Batch [20]#011Speed: 102.330 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:42:49 INFO 140248386783040] Epoch[22] Batch [40]#011Speed: 103.764 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:42:55 INFO 140248386783040] Epoch[22] Batch [60]#011Speed: 104.014 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:43:01 INFO 140248386783040] Epoch[22] Batch [80]#011Speed: 104.198 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:43:07 INFO 140248386783040] Epoch[22] Batch [100]#011Speed: 104.518 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:43:13 INFO 140248386783040] Epoch[22] Batch [120]#011Speed: 104.628 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:43:19 INFO 140248386783040] Epoch[22] Batch [140]#011Speed: 104.713 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:43:24 INFO 140248386783040] Epoch[22] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:43:24 INFO 140248386783040] Epoch[22] Time cost=47.273\u001b[0m\n",
      "\u001b[32m[08/07/2025 21:43:25 INFO 140248386783040] Epoch[22] Validation-accuracy=0.807292\u001b[0m\n",
      "\n",
      "2025-08-07 21:43:33 Uploading - Uploading generated training model\n",
      "2025-08-07 21:43:46 MaxWaitTimeExceeded - Training job wait time exceeded MaxWaitTimeInSeconds provided\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Job ended with status 'Stopped' rather than 'Completed'. This could mean the job timed out or stopped early for some other reason: Consider checking whether it completed as you expect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training seconds: 1153\n",
      "Billable seconds: 440\n",
      "Managed Spot Training savings: 61.8%\n"
     ]
    }
   ],
   "source": [
    "## TODO: train your model\n",
    "img_classifier_model.fit(inputs=model_inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all goes well, you'll end up with a model topping out above `.8` validation accuracy. With only 1000 training samples in the CIFAR dataset, that's pretty good. We could definitely pursue data augmentation & gathering more samples to help us improve further, but for now let's proceed to deploy our model.\n",
    "\n",
    "### Getting ready to deploy\n",
    "\n",
    "To begin with, let's configure Model Monitor to track our deployment. We'll define a `DataCaptureConfig` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    " \n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True,  # Enable data capture\n",
    "    sampling_percentage=100,  # Capture 100% of inferences\n",
    "    destination_s3_uri= f\"s3://{bucket}/scones-unlimited/data_capture\",\n",
    "    capture_options=[\"REQUEST\", \"RESPONSE\"],  # Capture both input and output\n",
    "    csv_content_types=[\"text/csv\"],  # For tabular data\n",
    "    json_content_types=[\"application/json\"]  # For JSON input/output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `destination_s3_uri` parameter: At the end of the project, we can explore the `data_capture` directory in S3 to find crucial data about the inputs and outputs Model Monitor has observed on our model endpoint over time.\n",
    "\n",
    "With that done, deploy your model on a single `ml.m5.xlarge` instance with the data capture config attached:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: scones-unlimited-2025-08-07-21-48-29-994\n",
      "INFO:sagemaker:Creating endpoint-config with name scones-unlimited-2025-08-07-21-48-29-994\n",
      "INFO:sagemaker:Creating endpoint with name scones-unlimited-2025-08-07-21-48-29-994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!scones-unlimited-2025-08-07-21-48-29-994\n"
     ]
    }
   ],
   "source": [
    "deployment = img_classifier_model.deploy(\n",
    "    ## TODO: fill in deployment options\n",
    "    initial_instance_count=1,           # Number of instances\n",
    "    instance_type='ml.m5.xlarge',       # Instance type\n",
    "    data_capture_config=data_capture_config\n",
    "    )\n",
    "\n",
    "endpoint = deployment.endpoint_name\n",
    "endpoint_name=\"scones-unlimited-2025-08-07-21-48-29-994\"\n",
    "print(endpoint)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the endpoint name for later as well.\n",
    "\n",
    "Next, instantiate a Predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import IdentitySerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "endpoint_name=\"scones-unlimited-2025-08-07-21-48-29-994\"\n",
    "# Create serializer and deserializer\n",
    "serializer = IdentitySerializer(content_type=\"application/x-image\")\n",
    "deserializer = JSONDeserializer()\n",
    "\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,  # Use the endpoint name you printed earlier\n",
    "    sagemaker_session=session,  # Your SageMaker session\n",
    "    serializer=serializer,   # How to encode input data\n",
    "    deserializer=deserializer  # How to decode output\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code snippet below we are going to prepare one of your saved images for prediction. Use the predictor to process the `payload`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import IdentitySerializer\n",
    "import base64\n",
    "\n",
    "predictor.serializer = IdentitySerializer(\"image/png\")\n",
    "with open(\"./test/bicycle_s_001789.png\", \"rb\") as f:\n",
    "    payload = f.read()\n",
    "\n",
    "    \n",
    "## TODO: Process the payload with your predictor\n",
    "inference = predictor.predict(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your `inference` object is an array of two values, the predicted probability value for each of your classes (bicycle and motorcycle respectively.) So, for example, a value of `b'[0.91, 0.09]'` indicates the probability of being a bike is 91% and being a motorcycle is 9%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.999966025352478, 3.3995274861808866e-05]\n"
     ]
    }
   ],
   "source": [
    "print(inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draft Lambdas and Step Function Workflow\n",
    "\n",
    "Your operations team uses Step Functions to orchestrate serverless workflows. One of the nice things about Step Functions is that [workflows can call other workflows](https://docs.aws.amazon.com/step-functions/latest/dg/connect-stepfunctions.html), so the team can easily plug your workflow into the broader production architecture for Scones Unlimited.\n",
    "\n",
    "In this next stage you are going to write and deploy three Lambda functions, and then use the Step Functions visual editor to chain them together. Our function are going to work with a simple data object\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"inferences\": [], # Output of predictor.predict\n",
    "    \"s3_key\": \"\", # Source data S3 key\n",
    "    \"s3_bucket\": \"\", # Source data S3 bucket\n",
    "    \"image_data\": \"\"  # base64 encoded string containing the image data\n",
    "}\n",
    "```\n",
    "\n",
    "A good test object that you can use for Lambda tests and Step Function executions, throughout the next section, might look like this:\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"image_data\": \"\",\n",
    "  \"s3_bucket\": MY_BUCKET_NAME, # Fill in with your bucket\n",
    "  \"s3_key\": \"test/bicycle_s_000513.png\"\n",
    "}\n",
    "```\n",
    "\n",
    "Using these fields, your functions can read and write the necessary data to execute your workflow. Let us start with the first function. Your first Lambda function will copy an object from S3, base64 encode it, and then return it to the step function as `image_data` in an event.\n",
    "\n",
    "Go to the Lambda dashboard and create a new Lambda function with a descriptive name like \"serializeImageData\" and select the 'Python 3.8' runtime. Add the same permissions as the SageMaker role you created earlier. (Reminder: you do this in the Configuration tab under \"Permissions\"). Once you are ready, use the starter code below to craft your Lambda handler:\n",
    "\n",
    "```python\n",
    "import json\n",
    "import boto3\n",
    "import base64\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"A function to serialize target data from S3\"\"\"\n",
    "    \n",
    "    # Get the s3 address from the Step Function event input\n",
    "    key = ## TODO: fill in\n",
    "    bucket = ## TODO: fill in\n",
    "    \n",
    "    # Download the data from s3 to /tmp/image.png\n",
    "    ## TODO: fill in\n",
    "    \n",
    "    # We read the data from a file\n",
    "    with open(\"/tmp/image.png\", \"rb\") as f:\n",
    "        image_data = base64.b64encode(f.read())\n",
    "\n",
    "    # Pass the data back to the Step Function\n",
    "    print(\"Event:\", event.keys())\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': {\n",
    "            \"image_data\": image_data,\n",
    "            \"s3_bucket\": bucket,\n",
    "            \"s3_key\": key,\n",
    "            \"inferences\": []\n",
    "        }\n",
    "    }\n",
    "```\n",
    "\n",
    "The next function is responsible for the classification part - we are going to take the image output from the previous function, decode it, and then pass inferences back to the the Step Function.\n",
    "\n",
    "Because this Lambda will have runtime dependencies (i.e. the SageMaker SDK) you will need to package them in your function. *Key reading:* https://docs.aws.amazon.com/lambda/latest/dg/python-package-create.html#python-package-create-with-dependency\n",
    "\n",
    "Create a new Lambda function with the same rights and a descriptive name, then fill in the starter code below for your classifier Lambda.\n",
    "\n",
    "```python\n",
    "import json\n",
    "import sagemaker\n",
    "import base64\n",
    "from sagemaker.serializers import IdentitySerializer\n",
    "\n",
    "# Fill this in with the name of your deployed model\n",
    "ENDPOINT = ## TODO: fill in\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "\n",
    "    # Decode the image data\n",
    "    image = base64.b64decode(## TODO: fill in)\n",
    "\n",
    "    # Instantiate a Predictor\n",
    "    predictor = ## TODO: fill in\n",
    "\n",
    "    # For this model the IdentitySerializer needs to be \"image/png\"\n",
    "    predictor.serializer = IdentitySerializer(\"image/png\")\n",
    "    \n",
    "    # Make a prediction:\n",
    "    inferences = ## TODO: fill in\n",
    "    \n",
    "    # We return the data back to the Step Function    \n",
    "    event[\"inferences\"] = inferences.decode('utf-8')\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps(event)\n",
    "    }\n",
    "```\n",
    "\n",
    "Finally, we need to filter low-confidence inferences. Define a threshold between 1.00 and 0.000 for your model: what is reasonble for you? If the model predicts at `.70` for itis highest confidence label, do we want to pass that inference along to downstream systems? Make one last Lambda function and tee up the same permissions:\n",
    "\n",
    "```python\n",
    "import json\n",
    "\n",
    "\n",
    "THRESHOLD = .93\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    # Grab the inferences from the event\n",
    "    inferences = ## TODO: fill in\n",
    "    \n",
    "    # Check if any values in our inferences are above THRESHOLD\n",
    "    meets_threshold = ## TODO: fill in\n",
    "    \n",
    "    # If our threshold is met, pass our data back out of the\n",
    "    # Step Function, else, end the Step Function with an error\n",
    "    if meets_threshold:\n",
    "        pass\n",
    "    else:\n",
    "        raise(\"THRESHOLD_CONFIDENCE_NOT_MET\")\n",
    "\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps(event)\n",
    "    }\n",
    "```\n",
    "Once you have tested the lambda functions, save the code for each lambda function in a python script called 'lambda.py'.\n",
    "\n",
    "With your lambdas in place, you can use the Step Functions visual editor to construct a workflow that chains them together. In the Step Functions console you will have the option to author a Standard step function *Visually*.\n",
    "\n",
    "When the visual editor opens, you will have many options to add transitions in your workflow. We are going to keep it simple and have just one: to invoke Lambda functions. Add three of them chained together. For each one, you will be able to select the Lambda functions you just created in the proper order, filter inputs and outputs, and give them descriptive names.\n",
    "\n",
    "Make sure that you:\n",
    "\n",
    "1. Are properly filtering the inputs and outputs of your invocations (e.g. `$.body`)\n",
    "2. Take care to remove the error handling from the last function - it is supposed to \"fail loudly\" for your operations colleagues!\n",
    "\n",
    "Take a screenshot of your working step function in action and export the step function as JSON for your submission package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first lambda\n",
    "import json\n",
    "import boto3\n",
    "import base64\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Get parameters from input event\n",
    "    key = event['s3_key']\n",
    "    bucket = event['s3_bucket']\n",
    "    \n",
    "    # Download image to temporary storage\n",
    "    s3.download_file(bucket, key, \"/tmp/image.png\")\n",
    "    \n",
    "    # Read and encode image\n",
    "    with open(\"/tmp/image.png\", \"rb\") as f:\n",
    "        image_data = base64.b64encode(f.read()).decode('utf-8')\n",
    "    \n",
    "    # Return processed data\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': {\n",
    "            \"image_data\": image_data,\n",
    "            \"s3_bucket\": bucket,\n",
    "            \"s3_key\": key,\n",
    "            \"inferences\": []\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import base64\n",
    "from sagemaker.serializers import IdentitySerializer\n",
    "\n",
    "ENDPOINT = \"your-endpoint-name-here\"  # << REPLACE WITH YOUR ENDPOINT NAME\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Decode base64 image data\n",
    "    image = base64.b64decode(event['body']['image_data'])\n",
    "    \n",
    "    # Initialize SageMaker client\n",
    "    runtime = boto3.client('sagemaker-runtime')\n",
    "    \n",
    "    # Make prediction\n",
    "    response = runtime.invoke_endpoint(\n",
    "        EndpointName=ENDPOINT,\n",
    "        ContentType='image/png',\n",
    "        Body=image\n",
    "    )\n",
    "    \n",
    "    # Parse response\n",
    "    inferences = json.loads(response['Body'].read().decode('utf-8'))\n",
    "    \n",
    "    # Update event with inferences\n",
    "    event['body'][\"inferences\"] = inferences\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': event['body']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "THRESHOLD = .93  # Confidence threshold\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Get inferences from event\n",
    "    inferences = event['body']['inferences']['predictions']  # Adjust based on your model output\n",
    "    \n",
    "    # Check threshold\n",
    "    meets_threshold = any(inference['probability'] >= THRESHOLD \n",
    "                          for inference in inferences)\n",
    "    \n",
    "    # Return or raise error\n",
    "    if meets_threshold:\n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'body': event['body']\n",
    "        }\n",
    "    else:\n",
    "        raise Exception(\"THRESHOLD_CONFIDENCE_NOT_MET\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Great! Now you can use the files in `./test` as test files for our workflow. Depending on our threshold, our workflow should reliably pass predictions about images from `./test` on to downstream systems, while erroring out for inferences below our confidence threshold!\n",
    "\n",
    "### Testing and Evaluation\n",
    "\n",
    "Do several step function invokations using data from the `./test` folder. This process should give you confidence that the workflow both *succeeds* AND *fails* as expected. In addition, SageMaker Model Monitor will generate recordings of your data and inferences which we can visualize.\n",
    "\n",
    "Here's a function that can help you generate test inputs for your invokations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"image_data\": \"\", \"s3_bucket\": \"sagemaker-us-west-2-869935066996\", \"s3_key\": \"scones-unlimited/test/cockroach_s_000163.png\"}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "\n",
    "def generate_test_case():\n",
    "    # Setup s3 in boto3\n",
    "    s3 = boto3.resource('s3')\n",
    "    \n",
    "    # Randomly pick from sfn or test folders in our bucket\n",
    "    # objects = s3.Bucket(bucket).objects.filter(\"test\")\n",
    "    objects = s3.Bucket(bucket).objects.filter(Prefix=\"scones-unlimited/test/\")\n",
    "    \n",
    "    # Grab any random object key from that folder!\n",
    "    obj = random.choice([x.key for x in objects])\n",
    "    \n",
    "    return json.dumps({\n",
    "        \"image_data\": \"\",\n",
    "        \"s3_bucket\": bucket,\n",
    "        \"s3_key\": obj\n",
    "    })\n",
    "# Generate and print a test case\n",
    "test_case = generate_test_case()\n",
    "print(test_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Step Function dashboard for your new function, you can create new executions and copy in the generated test cases. Do several executions so that you can generate data you can evaluate and visualize.\n",
    "\n",
    "Once you've done several executions, let's visualize the record of our inferences. Pull in the JSONLines data from your inferences like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['captured_data/AllTraffic/2025/08/06/21/40-46-541-865873a8-ee73-4e55-83c4-7a03df00d0e8.jsonl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "# In S3 your data will be saved to a datetime-aware path\n",
    "# Find a path related to a datetime you're interested in\n",
    "data_path =  \"s3://sagemaker-us-west-2-869935066996/scones-unlimited/data_capture/scones-unlimited-2025-08-06-21-11-22-300/\"\n",
    "\n",
    "S3Downloader.download(data_path, \"captured_data\")\n",
    "\n",
    "# Feel free to repeat this multiple times and pull in more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are in JSONLines format, where multiple valid JSON objects are stacked on top of eachother in a single `jsonl` file. We'll import an open-source library, `jsonlines` that was purpose built for parsing this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jsonlines\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonlines) (25.3.0)\n",
      "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Installing collected packages: jsonlines\n",
      "Successfully installed jsonlines-4.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install jsonlines\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract the data from each of the source files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./captured_data/AllTraffic/2025/08/06/21/40-46-541-865873a8-ee73-4e55-83c4-7a03df00d0e8.jsonl\n",
      "Total records loaded: 1\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "# # List the file names we downloaded\n",
    "# file_handles = os.listdir(\"./captured_data\")\n",
    "\n",
    "# # Dump all the data into an array\n",
    "# json_data = []\n",
    "# for jsonl in file_handles:\n",
    "#     with jsonlines.open(f\"./captured_data/{jsonl}\") as f:\n",
    "#         json_data.append(f.read())\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import jsonlines\n",
    "\n",
    "# Walk through all subdirectories to find .jsonl files\n",
    "json_data = []\n",
    "for root, dirs, files in os.walk(\"./captured_data\"):\n",
    "    for file in files:\n",
    "        if file.endswith('.jsonl'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            print(f\"Processing: {file_path}\")\n",
    "            with jsonlines.open(file_path) as f:\n",
    "                json_data.extend(f)  # Use extend to add all records from the file\n",
    "\n",
    "print(f\"Total records loaded: {len(json_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data should now be a list of dictionaries, with significant nesting. We'll give you an example of some code that grabs data out of the objects and visualizes it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.9696913361549377, 0.03030867874622345], '2025-08-06T21:40:46Z')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define how we'll get our data\n",
    "def simple_getter(obj):\n",
    "    inferences = obj[\"captureData\"][\"endpointOutput\"][\"data\"]\n",
    "    timestamp = obj[\"eventMetadata\"][\"inferenceTime\"]\n",
    "    return json.loads(inferences), timestamp\n",
    "\n",
    "simple_getter(json_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here's an example of a visualization you can build with this data. In this last part, you will take some time and build your own - the captured data has the input images, the resulting inferences, and the timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.font_manager:Matplotlib is building the font cache; this may take a moment.\n",
      "INFO:matplotlib.font_manager:generated new fontManager\n",
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHwCAYAAABOuNDeAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAaF5JREFUeJzt3XlYVNX/B/D3MOyyCKIIyOJKILkASkKKlnsuuORWKJmWpQXiSu6akgtmplAuuFVqpWYppmQuKCqBSy6IK0IIKoigIttwfn/w434dGVAQHXDer+eZp+bM5957zsww8/bcZWRCCAEiIiIiDaKl7g4QERERvWwMQERERKRxGICIiIhI4zAAERERkcZhACIiIiKNwwBEREREGocBiIiIiDQOAxARERFpHAYgIiIi0jgMQKSRHBwc4ODgoO5uVGt+fn6QyWRITExUd1dIhR9++AGtWrWCkZERZDIZZs+ere4uEdUoDEBU4x04cACDBw+Gra0t9PT0YG5ujjfffBNff/01cnNz1d09jZCYmAiZTKZ009HRgY2NDQYNGoTY2Fh1d7FKVTYcduzYETKZDGlpac+1/ejoaPj6+iInJwdjx47FrFmz0LFjx+daJ5Gm0VZ3B4gqq7CwEGPHjsWqVatQq1Yt9OjRA02aNEFWVhb27duHwMBAfPfdd9i9ezeaNGmi7u5qhMaNG+P9998HADx8+BBxcXH45Zdf8Ntvv+Gvv/5Chw4d1NzDV0NERAQAYOPGjXjjjTfU3BuimokBiGqsoKAgrFq1Cm3atMGOHTtgY2MjPaZQKDB37lzMnTsXPXr0QFxcHExMTNTYW83QpEmTUrtivvrqKwQFBWHGjBk4dOiQejr2irl58yYAoH79+mruCVHNxV1gVCNdvnwZS5cuhbm5Of744w+l8AMAcrkcc+bMwbBhw3DlyhUsWbJE5XoyMzMxevRoWFpawsDAAG3btsXvv/9eqi43NxchISFo2bIlTE1NYWRkhMaNG2Po0KE4e/ZsqfqdO3fi7bffhpmZGfT19eHi4oIlS5ZAoVAo1a1fvx4ymQzr16/H7t270b59exgbG8PBwQGHDx+GTCbDhx9+qLLv//33H+RyOd5++22l9vv372PWrFlo3rw5DAwMULt2bXTv3h1HjhxRuZ7z58+jV69eMDY2hqmpKXr27Ilz586prK2Mkv7HxcWVeiw/Px9Lly6Fq6sratWqBWNjY7Rv317la1BS/80336Bt27YwNjaGkZERnJ2dERgYiMzMTKXa27dvY/z48WjSpAn09PRgYWGBAQMGqBxbyTFhDx8+RGBgIGxsbKCnp4cWLVrg119/LVW7YcMGAEDDhg2lXX6V3QV18OBB6RiekydPolu3btJr0a9fP6XdbCW169atK7X9x/37778YMmQIrKysoKurC3t7e3z22WfIyMhQqivZdenn54eLFy+if//+sLCwKLV7rzLv5/379+PNN99ErVq1UKdOHYwYMaLU9h/v7/vvv48GDRpAT08PVlZW6N69O/74449Stc/al6KiIqxZswZt27aFubk5DA0N4eDgAB8fHxw+fPiprwtpAEFUA33xxRcCgJg6dWq5dfHx8QKAsLGxUWq3t7cXVlZWwtXVVTg5OYlJkyaJ0aNHC2NjYyGTycQPP/ygVD9o0CABQLRo0UL4+/uLyZMniyFDhghLS0uxbt06pdqgoCABQDRo0EB8+OGHYvz48cLNzU0AEAMHDlSqXbdunQAgevbsKbS1tYWPj4+YPHmy+OSTT0RRUZFwcHAQpqam4tGjR6XG9tVXXwkAStvPyMgQzZs3FwBE+/btxfjx48XIkSNFnTp1hLa2ttixY4fSOs6ePStMTEyElpaWGDhwoAgKChJvv/22MDExEe3btxcAxPXr18t9joUQ4vr16wKA6NatW6nHbt++LQAIU1NTpfbc3FzRsWNHAUC0bt1afPbZZ2LMmDHC1tZWABDffvutUv2jR49Ehw4dBADRtGlT8dlnn4mJEyeKvn37CgMDA3Hq1Cmp9sqVK6JBgwZCJpOJbt26iQkTJghfX19haGgoatWqJY4fP660bnt7e2FtbS08PT3Fa6+9JsaNGydGjhwpDA0NhUwmE3v37pVqv/76a9GyZUsBQPj7+4tZs2aJWbNmlXofqOLt7S0AiNTUVKntwIEDAoB45513hKGhoejZs6eYMGGCeOuttwQA0bhxY+n1v379upg1a5bK7ZfYuXOn0NPTE4aGhmLIkCFi0qRJ4p133pGet7t375Z63by8vISpqanw9PQUgYGBws/PT6SkpAghKvd+7t+/v9DV1RUDBgwQEyZMEG3atJG286Tt27cLPT09oaOjI/r37y+CgoLEhx9+KFxcXETfvn2VaivSl8mTJ0vP39ixY8XUqVOFr6+vcHBwUHq+SHMxAFGNVPLFGRkZ+dRaa2trAUAkJSVJbfb29gKAeOutt0R+fr7UHh8fLwwMDETt2rVFdna2EEKIe/fuCZlMJtzd3UVhYaHSugsLC0VmZqZ0f9++fQKA6NGjh3j48KHUXlRUJMaMGSMAiF9//VVqL/nCkMlkKscybdo0AUD8/PPPpR57/fXXhYGBgdRPIYQYNmyYACDCw8OVatPS0oStra2oW7euUpgq+UJ+MvCVfNFURQCaN2+e9AX/uJIQO3v2bFFUVCS1Z2dnC3d3d6Grqyt9CQshxKRJkwQA4evrW+p1uHfvnrh//75039PTU2hra4t9+/Yp1SUkJAhjY2Px+uuvK7WXvB/69u0r8vLypPa//vpL5bhGjBjxzM/N48oLQADEli1blOp9fX0FALF58+Zn2n56erowMTERDRo0EDdu3FB67KeffhIAxLhx46S2ktcNgJgxY0ap/lb2/aytrS2OHDkitRcWFkp/s8eOHZPab926JYyMjEStWrXEyZMnS20/OTm50n0xNzcXNjY2SrUl9RkZGaW2RZqHAYhqpNdee00AEBcvXnxqrYeHhwAgTpw4IbWVfOEdPXq0VP3YsWMFALFp0yYhhBBZWVll/uv1SX369CkVtkqUBKkBAwZIbSVfGP369VO5vosXLwoAok+fPkrtp0+fFgDEkCFDpLY7d+4IuVwu3n77bZXrWr58uQAg/vjjDyGEEDdu3JBmtZ50//59Ubt27QoHoMaNG0szEhMnTpS+8OvVqycuXLgg1SsUCmFmZiaaNGmiFH5K/P7770qzQIWFhcLExESYmpoqzWCocvLkSQFAfPjhhyofDwwMFADE2bNnpbaS98O1a9dK1dvb2wtzc3OlthcRgDp06FCqvuSxwMDAZ9r+0qVLld67T3J1dRUWFhbS/ZLXrX79+krBr0Rl38/Dhw8vVV/y2PLly6W2RYsWCQBi5syZKvv7PH0xNzcXDRs2VDkuIiGE4EHQ9MoTQgBAqeMkdHR0VJ5B0759e6xcuRKnT5/G+++/DxMTE3Tv3h1//vknXF1dMXDgQLRv3x4eHh7Q1dVVWvb48eOoVasW1q5dq7IvBgYGuHjxYqn2tm3bqqx3dHSEu7s79uzZg7t378Lc3BwAsGnTJgCAr6+vVPvPP/9AoVAgNzdX5TVhLl++DAC4ePEievXqhTNnzgAA3nzzzVK1RkZGaNWqFQ4ePKiyX2W5evUq5syZo9RWr149REVFoVmzZlJbQkICMjMzYW1tXaoeAO7cuSP1teS/2dnZ6Ny5M8zMzMrtw/HjxwEAaWlpKp+Hx9fp4uIitdeuXRsNGzYsVd+gQQMcO3as3G1WBVdXV5XbBoB79+490zpKxn78+HFcuXKl1OO5ublIT09Heno6LCwspPaWLVuWei+XrKcy7+dnHUtMTAwAoGvXruWMqnJ9GTRoEL777ju4uLhg8ODB8Pb2Rrt27VCrVq2nbos0AwMQ1Uj169fHxYsXkZycDEdHx3Jr//vvP2mZx9WpUwdaWqXPA7C0tAQAZGVlSW2//vorFixYgM2bN2PatGkAAGNjY4wcORILFiyAoaEhAODu3bsoLCxU+aVe4uHDh2VuUxVfX1/Exsbi559/xpgxY1BUVITNmzejXr16Sl8cd+/eBQAcPXoUR48efer2S8ZXr149lXXl9aks3bp1w59//gmgOMRs2LABU6ZMgY+PD2JiYmBkZKTU1/Pnz+P8+fNP7WvJl+aTB7urUrLu3bt3Y/fu3U9ddwlTU1OVddra2igqKnrqdp+Xqu1raxd/RD95gG9ZSsa+cuXKcusePnyoFIDKeq0r+35+1rFU9HWtSF+WL1+ORo0aYf369fjyyy/x5ZdfQl9fH4MGDUJISIjS+Ekz8SwwqpE8PT0BAPv37y+37uLFi7h58yZsbGxga2ur9FhGRobKL7Zbt24BUP4Qr1WrFubPn49r167h2rVrWLt2LV577TV88803GD9+vFRnYmKCOnXqQBTvXlZ5u379eqltPjk79bghQ4ZAW1sbP/zwAwDg77//xs2bNzF06FDpS6Vk2wAwYcKEcrc/a9YspfHdvn1b5XZLnofKqlu3LiZOnIgvvvgC8fHxmD59eqm+DhgwoNy+lpztVLt2bQBASkrKU7dbsu5vv/223HWPGDHiucZXHZWM/ezZs+WO3d7eXmm5st5/lX0/P6uKvq4V6YuOjg4mTZqE8+fPIyUlBT/99BPat2+PjRs34r333qt0n+nVwQBENdKIESOgpaWF1atXS7tLVJk/fz4AYOTIkaUeKygokHYZPC4qKgoA0KpVK5XrbNiwIUaOHIlDhw7ByMhI6ZRtDw8PZGRkSLubqkLJTE90dDSuX78uBaGSCw6WaNOmDWQy2TPvrmnZsiUAqDw9/sGDBzh9+vTzdfz/ffHFF7C2tkZoaKh0arWTkxNMTEwQGxuLgoKCp67D0dERJiYm+Oeff0qd7v4kDw8PAHihu63kcjmAZ5+ZeVmqeuwv4v38uJJdv/v27XuhfbG2tsbQoUPx559/omnTpvjrr7/w6NGjCq+HXi0MQFQjNWvWDP7+/sjIyEDv3r2Rmpqq9HhRURHmzZuHH374AY0bN8bEiRNVrmfGjBlKX8AXL15EeHg4TE1N0bdvXwDFu3JKjlV4XGZmJvLy8mBgYCC1ff755wCKA5eqa56kpaUhPj6+wuP19fWFEAJr1qzB9u3b8dprr8Hd3V2ppn79+hg0aBCio6OxePFi6dinx504cQI5OTkAADs7O3To0AH//vsvfvzxR6W6BQsWPPNxJ09jYGCAKVOmoKCgAPPmzQNQvDvkk08+wY0bNzBx4kSVIejcuXPS7JS2tjY+/vhjZGVlwd/fv1TwyMrKwoMHDwAUf6l6eHhg8+bN2Lp1a6n1FhUVPfcFGUuOxSrZvVpdfPDBBzA2Nsa0adNU7lrMyclRGfrL8qLezyVGjBgBIyMjhISEqAzcj88MVaQveXl5+Pvvv0v9DTx8+BD379+Hjo6OFGJJc/EYIKqxFi1ahKysLISHh6Np06Z455130LhxY2RnZ2Pfvn24fPkymjZtioiICJVXgbayssK9e/fQqlUrvPPOO8jKysLmzZuRm5uL1atXw9jYGEDxh7CHhweaN28OV1dX2NjYICMjAzt37kRBQQEmT54srbN79+6YMWMG5s2bhyZNmqB79+6wt7dHRkYGrly5gqioKHz55ZdwcnKq0Fj79u0LExMTLF68GAUFBUoHPz8uNDQUCQkJmDx5MjZt2oR27drB1NQUycnJiIuLw+XLl5Gamiods7Ry5Up4eXlh+PDh+O2339C0aVP8888/iImJQfv27aXZsOf10UcfYeHChdi4cSO++OILNG7cGHPmzMHJkyexfPly7N69G97e3qhbty5SUlJw9uxZnDlzBseOHZOOUZo7dy6OHz+OTZs24fjx4+jRowf09PRw7do1/Pnnnzhy5Ig0a7d582Z06tQJQ4YMwbJly+Dm5gZ9fX0kJSXh2LFjuHPnznP9Ttxbb72FJUuW4OOPP8a7776LWrVqwc7ODsOGDauKp6vS6tati82bN+Pdd99Fy5Yt0b17d7z22mvIzc3FjRs3cOjQIXh6ekrHaT3Ni3o/l6hXrx42btyIIUOGoG3btujTpw8cHR2Rnp6OEydOwMHBAb/99luF+/Lo0SO8/fbbaNSoETw8PGBnZ4cHDx5g165dSEtLw5QpU1Qe9E0apkrPKSNSg8jISPHuu+8Ka2troaOjI2rXri3atWsnQkJCRE5Ojspl7O3thb29vcjIyBCjRo0S9erVE3p6esLd3V3s3LlTqTYzM1PMnj1bdOjQQVhZWQldXV1hbW0tunfvrnSBvCf71Lt3b1G3bl2ho6Mj6tevL9q1ayfmzZundBpvyanBz3IRvQ8++EC6ZlBiYmKZdTk5OWLRokXCzc1N1KpVSxgYGIiGDRsKHx8fsXHjRlFQUKBUf/bsWdGzZ09hZGQkjI2NRY8ePcTZs2crdKp3edcBKvHtt99K1/EpUVhYKL7//nvh5eUlTExMhJ6enrCzsxPdu3cXYWFh4sGDB0rryM3NFUuWLBGtWrUSBgYGwsjISDg7O4sJEyYoXY9JCCHu3r0rpk+fLlxcXKTapk2bimHDhont27cr1Za8H1QpOXX9SYsWLRJNmzYVOjo6AoDw9vYu/0kS5Z8Gr+rifCXP64gRI5Tan/baXLx4UXz44YfC3t5e6OrqCjMzM/H666+Lzz//XMTExDx1/U+qivdzeeM8deqUGDRokLC0tBQ6OjrCyspK9OjRQ+zatatSfcnPzxcLFy4UXbt2FQ0aNBC6urrC0tJSeHt7l7rWEmkumRAq5smJiIiIXmE8BoiIiIg0DgMQERERaRwGICIiItI4DEBERESkcRiAiIiISOMwABEREZHGYQAiIiIijcMARERERBqHAYiIiIg0DgMQERERaRwGICIiItI4DEBERESkcRiAiIiISOMwAFGFzZ49G61atVJ3NyqkY8eOCAgIqJJ1JSYmQiaT4fTp0y9tm1VJCIGPPvoI5ubm0jiepa8ODg5YtmzZS+ljVVPXa1EVz5mfnx98fHzKrXmW8a1atQq2trbQ0tKqsa9jCZlMht9+++2FbuPgwYOQyWS4d+/eC90OqQ8DEJXi5+cHmUwGmUwGHR0dNGrUCBMnTsTDhw8BABMnTsT+/fufeX0v48PqZbK1tUVqaipcXFwA1LwPyj///BPr16/Hrl27pHFs374d8+bNU3fXnltNey1eluzsbIwbNw5TpkxBSkoKPvroI3V3qVpRFSA9PT2RmpoKU1NT9XSKXjhtdXeAqqfu3btj3bp1KCgoQFRUFEaNGoWHDx8iLCwMRkZGMDIyeul9ys/Ph66u7kvf7pPkcjnq16+v7m5U2tWrV2FlZQVPT0+pzdzcXI09qp4KCgqgo6Oj7m5UiaSkJBQUFOCdd96BlZVVpddTlc9JdX9+dXV1a/TfOT0dZ4BIJT09PdSvXx+2trYYNmwY3nvvPWkWR9UusPDwcDRv3hx6enqwsrLCuHHjABTvAgCAfv36QSaTSfdVTesHBASgY8eO0v2OHTti3LhxCAwMhIWFBbp06QIAuHDhAnr27AkjIyNYWlrC19cX6enp0nIPHz7E8OHDYWRkBCsrK4SEhJQ71qysLMjlcsTFxQEo3kVkbm6ONm3aSDWbN2+Wvjge3wWWmJiITp06AQDMzMwgk8ng5+cnLVdUVITJkyfD3Nwc9evXx+zZs8vtS3nPJVD8Rda3b18YGRnBxMQEgwYNwq1bt6THS16bTZs2wcHBAaamphgyZAju378PoPh5/+yzz5CUlKT0ejz5L+Dbt2+jd+/eMDAwQMOGDfHjjz+qfN4++ugj1KtXDyYmJnjrrbdw5syZZ+5LyfOzcOFCNGnSBHp6erCzs8P8+fOlx1NSUjB48GCYmZmhTp066Nu3LxITE1U+b8/7WshkMnz33Xfo27cvatWqhS+//BIA8Mcff8DNzQ36+vpo1KgR5syZg8LCQqVx2tnZQU9PD9bW1vj888+V1puTk4ORI0fC2NgYdnZ2WLVqldLjZ8+exVtvvQUDAwPUqVMHH330ER48eKByjEDF39/r16/H66+/DgBo1KgRZDKZ9ByGhYWhcePG0NXVhaOjIzZt2vRMz8mTHBwcMG/ePAwbNgxGRkawtrbGt99++0zrelofLl++jA4dOkBfXx/Ozs6IjIxUelzVrN/p06eVxgkAR48ehbe3NwwNDWFmZoZu3bohMzMTfn5+OHToEL755htp5jsxMVHlerdt2yb9bTo4OJR67h0cHLBgwYJyX29Vz92TuyRbtWql9P4s7z2Wn5+PyZMnw8bGBrVq1YKHhwcOHjxY7jbp/wmiJ4wYMUL07dtXqe2zzz4TderUEUIIMWvWLNGyZUvpsdDQUKGvry+WLVsmEhISRExMjPj666+FEELcvn1bABDr1q0Tqamp4vbt22Vuw9/fX3h7e0v3vb29hZGRkZg0aZK4ePGiiI+PFzdv3hQWFhYiKChIxMfHi5MnT4ouXbqITp06Sct98sknokGDBmLfvn3i33//Fb169RJGRkbC39+/zDG7urqKJUuWCCGEOH36tDAzMxO6uroiKytLCCHERx99JAYPHiyEEOL69esCgDh16pQoLCwU27ZtEwBEQkKCSE1NFffu3ZP6b2JiImbPni0uXbokNmzYIGQymdi3b1+Z/SjvuSwqKhKtW7cWb775poiNjRXHjx8Xrq6uSs/ZrFmzhJGRkejfv784e/asOHz4sKhfv7744osvhBBC3Lt3T8ydO1c0aNBA6fXw9vZWen569OghXFxcRHR0tIiNjRWenp7CwMBAqS9eXl6id+/e4p9//hGXLl0SEyZMEHXq1BEZGRnP1BchhJg8ebIwMzMT69evF1euXBFRUVFi9erVQgghHj58KJo2bSpGjhwp/v33X3HhwgUxbNgw4ejoKPLy8ko9d8/7WgAQ9erVE2vXrhVXr14ViYmJ4s8//xQmJiZi/fr14urVq2Lfvn3CwcFBzJ49WwghxC+//CJMTExERESEuHHjhjhx4oRYtWqVtE57e3thbm4uVq5cKS5fviyCg4OFlpaWiI+Pl8ZobW0tPUf79+8XDRs2FCNGjJDW8eTfSkXf3zk5OeKvv/4SAERMTIxITU0VhYWFYvv27UJHR0esXLlSJCQkiJCQECGXy8Xff/9d7nOiir29vTA2NhbBwcEiISFBLF++XMjl8qc+v0/rg0KhEC4uLqJjx47i1KlT4tChQ6J169YCgNixY4cQQogDBw4IACIzM1Pa1qlTpwQAcf36dem+np6e+OSTT8Tp06fFuXPnxLfffivu3Lkj7t27J9q1aydGjx4tUlNTpefnyfXGxsYKLS0tMXfuXJGQkCDWrVsnDAwMxLp165759S7ruSv5uyrRsmVLMWvWLCHE099jw4YNE56enuLw4cPiypUrYvHixUJPT09cunSpzG1SMQYgKuXJD9wTJ06IOnXqiEGDBgkhSgcga2trMW3atDLX9/iHVVnbEEJ1AGrVqpVSzYwZM0TXrl2V2pKTk6Uvvfv37wtdXV2xZcsW6fGMjAxhYGBQbgAKDAwUvXr1EkIIsWzZMjFw4EDh6uoqdu/eLYQQolmzZiIsLEwIoRyAhFD9AVzS/zfffFOprU2bNmLKlCll9qO853Lfvn1CLpeLpKQkqe38+fPSF5sQxa+NoaGhyM7OlmomTZokPDw8pPtff/21sLe3L9XXkucnISFBABDHjx+XHo+PjxcApA/q/fv3CxMTE5Gbm6u0nsaNG4vvv//+mfqSnZ0t9PT0pMDzpLVr1wpHR0dRVFQkteXl5QkDAwOxd+9elcs8z2sBQAQEBCjVtG/fXixYsECpbdOmTcLKykoIIURISIho1qyZyM/PV9kfe3t78f7770v3i4qKRL169aT30qpVq4SZmZl48OCBVLN7926hpaUl0tLShBDKfyuVfX8/GQiEEMLT01OMHj1aqe7dd98VPXv2LPc5KWuc3bt3V2obPHiw6NGjR7nrelof9u7dK+RyuUhOTpYe37NnT4UD0NChQ4WXl1eZ/X/yHwCq1jts2DDRpUsXpZpJkyYJZ2dnpeehvNdblacFoPLeY1euXBEymUykpKQotb/99tsiKCiozG1SMe4CI5V27doFIyMj6Ovro127dujQoUOpKW2geFfJzZs38fbbb7+Qfri7uyvdj4uLw4EDB6TjkIyMjPDaa68BKD625erVq8jPz0e7du2kZczNzeHo6Fjudjp27IioqCgUFRXh0KFD6NixIzp27IhDhw4hLS0Nly5dgre3d4X736JFC6X7VlZWuH37tsrapz2X8fHxsLW1ha2trdTm7OyM2rVrIz4+XmpzcHCAsbHxM22zrO1oa2srPfevvfYaateuLd2Pi4vDgwcPUKdOHaXX4vr167h69eoz9SU+Ph55eXlljjcuLg5XrlyBsbGxtH5zc3Pk5uYqbeNZPctroer9NnfuXKUxjh49GqmpqcjJycG7776LR48eoVGjRhg9ejR27NihtHvsye3KZDLUr19f6Tlo2bIlatWqJdV4eXmhqKgICQkJpcZQ2fe3KvHx8fDy8lJq8/LyUnovAaWfk7I83qeS+09b19P6EB8fDzs7OzRo0KDM7TyL06dPP/dnVFl9vXz5MhQKhdRW3utdGeW9x06ePAkhBJo1a6b0Hj106FCl/kY0DQ+CJpU6deqEsLAw6OjowNrausyDFQ0MDCq1fi0tLQghlNoKCgpK1T3+xQAUH8fRu3dvLFy4sFStlZUVLl++XKn+dOjQAffv38fJkycRFRWFefPmwdbWFgsWLECrVq1Qr149ODk5VXi9Tz5vMpkMRUVFKmuf9lwKISCTyZ7aXpFtlrWdkuXKUlRUBCsrK5XHGjwelMrry9PGW1RUBDc3N5XHH9WtW7fcZVV5ludF1fttzpw56N+/f6n16evrw9bWFgkJCYiMjMRff/2FTz/9FIsXL8ahQ4ek7ZW33bJe05K6Jz35N/O8ntyGqv48+Zw8z/pVrau8Pqga75P1WlpapWqf/Cyp7OdUWf16vO1JFf37e9pnYXnvsaKiIun4RblcrrQOdZyoUtNwBohUqlWrFpo0aQJ7e/tyz9QwNjaGg4NDuafF6+joKP0LCSj+AktNTVVqe9p1dQDA1dUV58+fh4ODA5o0aaJ0K+mzjo4Ojh8/Li2TmZmJS5culbteU1NTtGrVCitWrIBMJoOzszPat2+PU6dOYdeuXeXO/pScmfbkGCvqac+ls7MzkpKSkJycLLVduHABWVlZlQpnZXFyckJhYSFiY2OltoSEBKWDQV1dXZGWlgZtbe1Sr4OFhcUzbadp06YwMDAoc7yurq64fPky6tWrV2obZZ2aXFWvxeN9SEhIKLX9Jk2aSF+8BgYG6NOnD5YvX46DBw/i2LFjOHv27DOt39nZGadPn5YuMQEUH6yrpaWFZs2alaqv7PtbFScnJxw5ckSpLTo6utLvpcf7VHK/ZHa2sn0oec/fvHlTevzYsWNK9SVh+PHPkyc/S1q0aFHuZ5Suru5T3zPOzs4q+9qsWbNS4aMinvwszM7OxvXr15VqynqPtW7dGgqFArdv3y71/uQZbE/HAETPbfbs2QgJCcHy5ctx+fJlnDx5Uml3WcmXelpaGjIzMwEAb731FmJjY7Fx40ZcvnwZs2bNwrlz5566rbFjx+Lu3bsYOnQoYmJicO3aNezbtw8jR46EQqGAkZERPvzwQ0yaNAn79+/HuXPn4OfnJ31Zladjx4744Ycf4O3tDZlMBjMzMzg7O2Pr1q1KZ6c9yd7eHjKZDLt27cKdO3fKPYPnacp7Ljt37owWLVrgvffew8mTJxETE4Phw4fD29v7mXdTPAtHR0d0794do0ePxokTJxAXF4dRo0Yp/Su6c+fOaNeuHXx8fLB3714kJiYiOjoa06dPVwpO5dHX18eUKVMwefJkbNy4EVevXsXx48exdu1aAMB7770HCwsL9O3bF1FRUbh+/ToOHToEf39//PfffyrXWZWvBQDMnDkTGzduxOzZs3H+/HnEx8dj69atmD59OoDiM6zWrl2Lc+fO4dq1a9i0aRMMDAxgb2//TOt/7733oK+vjxEjRuDcuXM4cOAAPvvsM/j6+sLS0rJU/fO8v580adIkrF+/Ht999x0uX76MpUuXYvv27Zg4cWKF1wUUB7dFixbh0qVLWLlyJX755Rf4+/s/Vx86d+4MR0dHDB8+HGfOnEFUVBSmTZumtI4mTZrA1tYWs2fPxqVLl7B79+5SZ2cFBQXhn3/+waeffop///0XFy9eRFhYmHT2qIODA06cOIHExESkp6ernLGZMGEC9u/fj3nz5uHSpUvYsGEDVqxYUennq8Rbb72FTZs2ISoqCufOncOIESOUAlV577FmzZrhvffew/Dhw7F9+3Zcv34d//zzDxYuXIiIiIjn6pcmYACi5zZixAgsW7YMoaGhaN68OXr16qW0KyokJASRkZGwtbVF69atAQDdunXDjBkzMHnyZLRp0wb379/H8OHDn7ota2trHD16FAqFAt26dYOLiwv8/f1hamoqfQksXrwYHTp0QJ8+fdC5c2e8+eabcHNze+q6O3XqBIVCoRR2vL29oVAoyp0BsrGxwZw5czB16lRYWloqnbZeUeU9lyUXlDQzM0OHDh3QuXNnNGrUCFu3bq309sqybt062NrawtvbG/3795dOdy8hk8kQERGBDh06YOTIkWjWrBmGDBmCxMRElV/cZZkxYwYmTJiAmTNnwsnJCYMHD5aOlzA0NMThw4dhZ2eH/v37w8nJCSNHjsSjR49gYmKicn1V+VoAxe/TXbt2ITIyEm3atMEbb7yBpUuXSgGndu3aWL16Nby8vKRZhj/++AN16tR5pvUbGhpi7969uHv3Ltq0aYOBAwfi7bffxooVK8pcprLv7yf5+Pjgm2++weLFi9G8eXN8//33WLduXblhvzwTJkxAXFwcWrdujXnz5iEkJATdunV7rj5oaWlhx44dyMvLQ9u2bTFq1CilyyQAxTPMmzdvxsWLF9GyZUssXLiw1On6zZo1w759+3DmzBm0bdsW7dq1w86dO6GtXXwUyMSJEyGXy+Hs7Iy6desiKSmpVF9dXV3x888/Y8uWLXBxccHMmTMxd+5cpUstVEZQUBA6dOiAXr16oWfPnvDx8UHjxo2lx5/2Hlu3bh2GDx+OCRMmwNHREX369MGJEyeUjhUk1WSiqncqExGRRnFwcEBAQEC1/OkXorJwBoiIiIg0DgMQERERaRzuAiMiIiKNwxkgIiIi0jgMQERERKRxeCVoFYqKinDz5k0YGxuXezVcIiIiqj6EELh//z6sra2fen0sBiAVbt68yWsoEBER1VDJyclKvyGnCgOQCiU/3picnFzmBdeIiIioesnOzoatra3SjzCXhQFIhZLdXiYmJgxARERENcyzHL6i1oOgDx8+jN69e8Pa2lq6zP/THDp0CG5ubtDX10ejRo3w3XffKT2+evVqtG/fHmZmZjAzM0Pnzp0RExPzgkZARERENZFaA9DDhw/RsmXLcn/35nHXr19Hz549pV/p/uKLL/D5559j27ZtUs3BgwcxdOhQHDhwAMeOHYOdnR26du2KlJSUFzUMIiIiqmGqzYUQZTIZduzYAR8fnzJrpkyZgt9//x3x8fFS25gxY3DmzBkcO3ZM5TIKhQJmZmZYsWLFM/3YJlC8D9HU1BRZWVncBUZERFRDVOT7u0ZdB+jYsWPo2rWrUlu3bt0QGxuLgoIClcvk5OSgoKAA5ubmZa43Ly8P2dnZSjciIiJ6ddWoAJSWlgZLS0ulNktLSxQWFiI9PV3lMlOnToWNjQ06d+5c5nqDg4Nhamoq3XgKPBER0autRgUgoPSR3SV78FQd8b1o0SJs3rwZ27dvh76+fpnrDAoKQlZWlnRLTk6u2k4TERFRtVKjToOvX78+0tLSlNpu374NbW1t1KlTR6l9yZIlWLBgAf766y+0aNGi3PXq6elBT0+vyvtLRERE1VONmgFq164dIiMjldr27dsHd3d36OjoSG2LFy/GvHnz8Oeff8Ld3f1ld5OIiIiqObUGoAcPHuD06dM4ffo0gOLT3E+fPo2kpCQAxbumHj9za8yYMbhx4wYCAwMRHx+P8PBwrF27FhMnTpRqFi1ahOnTpyM8PBwODg5IS0tDWloaHjx48FLHRkRERNWXWk+DP3jwIDp16lSqfcSIEVi/fj38/PyQmJiIgwcPSo8dOnQI48ePx/nz52FtbY0pU6ZgzJgx0uMODg64ceNGqXXOmjULs2fPfqZ+8TR4IiKimqci39/V5jpA1QkDEBERUc3zyl4HiIiIiKgqMAARERGRxmEAIiIiIo3DAEREREQahwGIiIiINA4DEBEREWkcBiAiIiLSOAxAREREpHEYgIiIiEjjMAARERGRxmEAIiIiIo3DAEREREQahwGIiIiINA4DEBEREWkcBiAiIiLSOAxAREREpHEYgIiIiEjjMAARERGRxmEAIiIiIo3DAEREREQahwGIiIiINA4DEBEREWkcBiAiIiLSOAxAREREpHEYgIiIiEjjMAARERGRxmEAIiIiIo3DAEREREQahwGIiIiINA4DEBEREWkcBiAiIiLSOAxAREREpHEYgIiIiEjjMAARERGRxmEAIiIiIo2j1gB0+PBh9O7dG9bW1pDJZPjtt9+eusyhQ4fg5uYGfX19NGrUCN99912pmm3btsHZ2Rl6enpwdnbGjh07XkDviYiIqKZSawB6+PAhWrZsiRUrVjxT/fXr19GzZ0+0b98ep06dwhdffIHPP/8c27Ztk2qOHTuGwYMHw9fXF2fOnIGvry8GDRqEEydOvKhhEBERUQ0jE0IIdXcCAGQyGXbs2AEfH58ya6ZMmYLff/8d8fHxUtuYMWNw5swZHDt2DAAwePBgZGdnY8+ePVJN9+7dYWZmhs2bNz9TX7Kzs2FqaoqsrCyYmJhUbkBERET0UlXk+7tGHQN07NgxdO3aVamtW7duiI2NRUFBQbk10dHRZa43Ly8P2dnZSjciIiJ6ddWoAJSWlgZLS0ulNktLSxQWFiI9Pb3cmrS0tDLXGxwcDFNTU+lma2tb9Z0nIiKiaqNGBSCgeFfZ40r24D3erqrmybbHBQUFISsrS7olJydXYY+JiIioutFWdwcqon79+qVmcm7fvg1tbW3UqVOn3JonZ4Uep6enBz09varvMBEREVVLNWoGqF27doiMjFRq27dvH9zd3aGjo1Nujaen50vrJxEREVVvap0BevDgAa5cuSLdv379Ok6fPg1zc3PY2dkhKCgIKSkp2LhxI4DiM75WrFiBwMBAjB49GseOHcPatWuVzu7y9/dHhw4dsHDhQvTt2xc7d+7EX3/9hSNHjrz08REREVH1pNYZoNjYWLRu3RqtW7cGAAQGBqJ169aYOXMmACA1NRVJSUlSfcOGDREREYGDBw+iVatWmDdvHpYvX44BAwZINZ6entiyZQvWrVuHFi1aYP369di6dSs8PDxe7uCIiIio2qo21wGqTngdICIioprnlb0OEBEREVFVYAAiIiIijcMARERERBqHAYiIiIg0DgMQERERaRwGICIiItI4DEBERESkcRiAiIiISOMwABEREZHGYQAiIiIijcMARERERBqHAYiIiIg0DgMQERERaRwGICIiItI4DEBERESkcRiAiIiISOMwABEREZHGYQAiIiIijcMARERERBqHAYiIiIg0DgMQERERaRwGICIiItI4DEBERESkcRiAiIiISOMwABEREZHGYQAiIiIijcMARERERBqHAYiIiIg0DgMQERERaRwGICIiItI4DEBERESkcRiAiIiISOMwABEREZHGYQAiIiIijcMARERERBpH7QEoNDQUDRs2hL6+Ptzc3BAVFVVu/cqVK+Hk5AQDAwM4Ojpi48aNpWqWLVsGR0dHGBgYwNbWFuPHj0dubu6LGgIRERHVMNrq3PjWrVsREBCA0NBQeHl54fvvv0ePHj1w4cIF2NnZlaoPCwtDUFAQVq9ejTZt2iAmJgajR4+GmZkZevfuDQD48ccfMXXqVISHh8PT0xOXLl2Cn58fAODrr79+mcMjIiKiakomhBDq2riHhwdcXV0RFhYmtTk5OcHHxwfBwcGl6j09PeHl5YXFixdLbQEBAYiNjcWRI0cAAOPGjUN8fDz2798v1UyYMAExMTFlzi7l5eUhLy9Pup+dnQ1bW1tkZWXBxMTkucdJREREL152djZMTU2f6ftbbbvA8vPzERcXh65duyq1d+3aFdHR0SqXycvLg76+vlKbgYEBYmJiUFBQAAB48803ERcXh5iYGADAtWvXEBERgXfeeafMvgQHB8PU1FS62draPs/QiIiIqJpTWwBKT0+HQqGApaWlUrulpSXS0tJULtOtWzesWbMGcXFxEEIgNjYW4eHhKCgoQHp6OgBgyJAhmDdvHt58803o6OigcePG6NSpE6ZOnVpmX4KCgpCVlSXdkpOTq26gREREVO2o9RggAJDJZEr3hRCl2krMmDEDaWlpeOONNyCEgKWlJfz8/LBo0SLI5XIAwMGDBzF//nyEhobCw8MDV65cgb+/P6ysrDBjxgyV69XT04Oenl7VDoyIiIiqLbXNAFlYWEAul5ea7bl9+3apWaESBgYGCA8PR05ODhITE5GUlAQHBwcYGxvDwsICQHFI8vX1xahRo/D666+jX79+WLBgAYKDg1FUVPTCx0VERETVn9oCkK6uLtzc3BAZGanUHhkZCU9Pz3KX1dHRQYMGDSCXy7Flyxb06tULWlrFQ8nJyZH+v4RcLocQAmo83puIiIiqEbXuAgsMDISvry/c3d3Rrl07rFq1CklJSRgzZgyA4mNzUlJSpGv9XLp0CTExMfDw8EBmZiaWLl2Kc+fOYcOGDdI6e/fujaVLl6J169bSLrAZM2agT58+0m4yIiIi0mxqDUCDBw9GRkYG5s6di9TUVLi4uCAiIgL29vYAgNTUVCQlJUn1CoUCISEhSEhIgI6ODjp16oTo6Gg4ODhINdOnT4dMJsP06dORkpKCunXronfv3pg/f/7LHh4RERFVU2q9DlB1VZHrCBAREVH1UCOuA0RERESkLgxAREREpHEYgIiIiEjjMAARERGRxmEAIiIiIo3DAEREREQahwGIiIiINA4DEBEREWkcBiAiIiLSOAxAREREpHEYgIiIiEjjqPXHUImIXiaFAoiKAlJTASsroH17QC5Xd6+ISB0YgIhII2zfDvj7A//997+2Bg2Ab74B+vdXX7+ISD24C4yIXnnbtwMDByqHHwBISSlu375dPf0iIvVhACKiV5pCUTzzI0Tpx0raAgKK64hIczAAEdErLSqq9MzP44QAkpOL64hIczAAEdErLTW1auuI6NXAAERErzQrq6qtI6JXAwMQEb3S2rcvPttLJlP9uEwG2NoW1xGR5mAAIqJXmlxefKo7UDoEldxftozXAyLSNAxARPTK698f+PVXwMZGub1Bg+J2XgeISPNU+kKIhYWFOHjwIK5evYphw4bB2NgYN2/ehImJCYyMjKqyj0REz61/f6BvX14JmoiKVSoA3bhxA927d0dSUhLy8vLQpUsXGBsbY9GiRcjNzcV3331X1f0kInpucjnQsaO6e0FE1UGldoH5+/vD3d0dmZmZMDAwkNr79euH/fv3V1nniIiIiF6ESs0AHTlyBEePHoWurq5Su729PVJSUqqkY0REREQvSqVmgIqKiqBQcd34//77D8bGxs/dKSIiIqIXqVIBqEuXLli2bJl0XyaT4cGDB5g1axZ69uxZVX0jIiIieiFkQqj6icDy3bx5E506dYJcLsfly5fh7u6Oy5cvw8LCAocPH0a9evVeRF9fmuzsbJiamiIrKwsmJibq7g4RERE9g4p8f1fqGCBra2ucPn0aW7ZsQVxcHIqKivDhhx/ivffeUzoomoiIiKg6qtQM0KuOM0BEREQ1T0W+vyt1DFBwcDDCw8NLtYeHh2PhwoWVWSURERHRS1OpAPT999/jtddeK9XevHlzXgSRiIiIqr1KBaC0tDRYWVmVaq9bty5SU1Ofu1NEREREL1KlApCtrS2OHj1aqv3o0aOwtrZ+7k4RERERvUiVCkCjRo1CQEAA1q1bhxs3buDGjRsIDw/H+PHjMXr06AqtKzQ0FA0bNoS+vj7c3NwQFRVVbv3KlSvh5OQEAwMDODo6YuPGjaVq7t27h7Fjx8LKygr6+vpwcnJCREREhfpFREREr65KnQY/efJk3L17F59++iny8/MBAPr6+pgyZQqCgoKeeT1bt25FQEAAQkND4eXlhe+//x49evTAhQsXYGdnV6o+LCwMQUFBWL16Ndq0aYOYmBiMHj0aZmZm6N27NwAgPz8fXbp0Qb169fDrr7+iQYMGSE5O5hWqiYiISPJcp8E/ePAA8fHxMDAwQNOmTaGnp1eh5T08PODq6oqwsDCpzcnJCT4+PggODi5V7+npCS8vLyxevFhqCwgIQGxsLI4cOQIA+O6777B48WJcvHgROjo6lRoXT4MnIiKqeV74afAljIyM0KZNG7i4uFQ4/OTn5yMuLg5du3ZVau/atSuio6NVLpOXlwd9fX2lNgMDA8TExKCgoAAA8Pvvv6Ndu3YYO3YsLC0t4eLiggULFqj87bLH15udna10IyIioldXpQLQw4cPMWPGDHh6eqJJkyZo1KiR0u1ZpKenQ6FQwNLSUqnd0tISaWlpKpfp1q0b1qxZg7i4OAghEBsbi/DwcBQUFCA9PR0AcO3aNfz6669QKBSIiIjA9OnTERISgvnz55fZl+DgYJiamko3W1vbZ3wmiIiIqCaq1DFAo0aNwqFDh+Dr6wsrKyvIZLJKd+DJZYUQZa5vxowZSEtLwxtvvAEhBCwtLeHn54dFixZBLpcDKP6l+nr16mHVqlWQy+Vwc3PDzZs3sXjxYsycOVPleoOCghAYGCjdz87OZggiIiJ6hVUqAO3Zswe7d++Gl5dXpTdsYWEBuVxearbn9u3bpWaFShgYGCA8PBzff/89bt26BSsrK6xatQrGxsawsLAAAFhZWUFHR0cKREDxcUVpaWnIz8+Hrq5uqfXq6elVeBceERER1VyV2gVmZmYGc3Pz59qwrq4u3NzcEBkZqdQeGRkJT0/PcpfV0dFBgwYNIJfLsWXLFvTq1QtaWsVD8fLywpUrV1BUVCTVX7p0CVZWVirDDxEREWmeSgWgefPmYebMmcjJyXmujQcGBmLNmjUIDw9HfHw8xo8fj6SkJIwZMwZA8a6p4cOHS/WXLl3CDz/8gMuXLyMmJgZDhgzBuXPnsGDBAqnmk08+QUZGBvz9/XHp0iXs3r0bCxYswNixY5+rr0RERPTqqNQusJCQEFy9ehWWlpZwcHAodbr5yZMnn2k9gwcPRkZGBubOnYvU1FS4uLggIiIC9vb2AIDU1FQkJSVJ9QqFAiEhIUhISICOjg46deqE6OhoODg4SDW2trbYt28fxo8fjxYtWsDGxgb+/v6YMmVKZYZKREREr6BKXQdozpw55T4+a9asSneoOuB1gIiIiGqeinx/P9eFEF9VDEBEREQ1z0u5EOK9e/ewZs0aBAUF4e7duwCKd32lpKRUdpVEREREL0WljgH6999/0blzZ5iamiIxMRGjR4+Gubk5duzYgRs3bqj8gVIiIiKi6qJSM0CBgYHw8/PD5cuXlX6aokePHjh8+HCVdY6IiIjoRahUAPrnn3/w8ccfl2q3sbEp82csiIiIiKqLSgUgfX19lT8YmpCQgLp16z53p4iIiIhepEoFoL59+2Lu3LnSL7DLZDIkJSVh6tSpGDBgQJV2kIiIiKiqVSoALVmyBHfu3EG9evXw6NEjeHt7o0mTJjA2Ni73V9eJiIiIqoNKnQVmYmKCI0eO4O+//8bJkydRVFQEV1dXdO7cuar7R0RERFTleCFEFXghRCIiopqnIt/fzzwDtHz58mfuwOeff/7MtUREREQv2zPPADVs2FDp/p07d5CTk4PatWsDKL4ytKGhIerVq4dr165VeUdfJs4AERER1Twv5Kcwrl+/Lt3mz5+PVq1aIT4+Hnfv3sXdu3cRHx8PV1dXzJs377kHQERERPQiVeoYoMaNG+PXX39F69atldrj4uIwcOBAXL9+vco6qA6cASIiIqp5XviPoaampkrXAHqcQqHArVu3KrNKIiIiopemUgHo7bffxujRoxEbG4uSCaTY2Fh8/PHHPBWeiIiIqr1KBaDw8HDY2Nigbdu20NfXh56eHjw8PGBlZYU1a9ZUdR+JiIiIqlSlLoRYt25dRERE4NKlS7h48SKEEHByckKzZs2qun9EREREVa5SAahEs2bNGHqIiIioxnnmABQYGIh58+ahVq1aCAwMLLd26dKlz90xIiIiohflmQPQqVOnpDO/Tp48CZlMprKurHYiIiKi6uKZA9A333wjnVN/8ODBF9UfIiIiohfumc8Ca926NdLT0wEAjRo1QkZGxgvrFBEREdGL9MwBqHbt2tIVnhMTE1FUVPTCOkVERET0Ij3zLrABAwbA29sbVlZWkMlkcHd3h1wuV1lb038MlYiIiF5tzxyAVq1ahf79++PKlSv4/PPPMXr0aBgbG7/IvhERERG9EBW6DlD37t0BFP/oqb+/PwMQERER1UiVuhDiunXrqrofRERERC9NpQLQw4cP8dVXX2H//v24fft2qQOieQwQERERVWeVCkCjRo3CoUOH4OvrKx0UTURERFRTVCoA7dmzB7t374aXl1dV94eIiIjohXvm6wA9zszMDObm5lXdFyIiIqKXolIBaN68eZg5cyZycnKquj9EREREL1yldoGFhITg6tWrsLS0hIODA3R0dJQeP3nyZJV0joiIiOhFqFQA8vHxqeJuEBEREb08MiGEUGcHQkNDsXjxYqSmpqJ58+ZYtmwZ2rdvX2b9ypUrsWLFCiQmJsLOzg7Tpk3D8OHDVdZu2bIFQ4cORd++ffHbb789c5+ys7NhamqKrKwsmJiYVHRIREREpAYV+f6u1AxQibi4OMTHx0Mmk8HZ2RmtW7eu0PJbt25FQEAAQkND4eXlhe+//x49evTAhQsXYGdnV6o+LCwMQUFBWL16Ndq0aYOYmBiMHj0aZmZm6N27t1LtjRs3MHHixHLDFBEREWmmSs0A3b59G0OGDMHBgwdRu3ZtCCGQlZWFTp06YcuWLahbt+4zrcfDwwOurq4ICwuT2pycnODj44Pg4OBS9Z6envDy8sLixYultoCAAMTGxuLIkSNSm0KhgLe3Nz744ANERUXh3r17lZoBunnnpsoEKdeSQ19bX7r/MP9hmevSkmnBQMegUrU5BTko6+WRyWQw1DGsVO2jgkcoEkUqawGglm6tStXmFuZCUaSoklpDHUPp+lJ5hXkoLCqskloDHQNoyYqP/c9X5KNAUVAltfra+pBryStcW6AoQL4iv8xaPW09aGtpV7i2sKgQeYV5ZdbqynWhI9epcK2iSIHcwtwya3XkOtCV61a4tkgU4VHBoyqp1dbShp62HgBACIGcgrJP1qhIbUX+7vkZobqWnxH8jHjRnxEvfAbos88+Q3Z2Ns6fPw8nJycAwIULFzBixAh8/vnn2Lx581PXkZ+fj7i4OEydOlWpvWvXroiOjla5TF5eHvT19ZXaDAwMEBMTg4KCAulg7Llz56Ju3br48MMPERUV9dS+5OXlIS/vfy9udnY2AMA6xBrQL13fs2lP7B62W7pfb0m9Mj84ve29cdDvoHTf4RsHpOekq6x1t3bHP6P/ke47r3TGjawbKmud6zrj/KfnpfttVrfBhTsXVNbam9ojMSBRut9hfQfE3oxVWWthaIE7k+5I93v82AOHbhxSWWuoY4iHX/zvw3rAzwMQcTlCZS0AiFn/+/D13eGLXy/8Wmbtg6AH0ofhx7s+xoYzG8qsvT3xNurWKg7dgXsDERobWmbtdf/rcKjtAACYtn8alhxbUmbtuU/OoXm95gCABVELMOfQnDJrY0bFoI1NGwDAN8e/weS/JpdZe2DEAXR06AgAWBW3CuP2jCuzdtfQXXin2TsAgB/P/ogPdn5QZu3PA3/Gu83fBQDsiN+BQb8OKrN2Xd918GvlBwDYe2Uvem3uVWbtih4rMLbtWABAVFIUOm3oVGbtos6LMMlrEgDgZOpJtF3TtszaWd6zMLvjbABA/J14uIS5lFk7sd1ELO5a/A+fpKwkNPymYZm1n7p/ipXvrAQApOeko96SemXWjmg5Aut91gMoDghGwUZl1g50Hohf3v1Ful9eLT8jivEz4n/4GVHsZXxGPKtKnQb/559/IiwsTAo/AODs7IyVK1diz549z7SO9PR0KBQKWFpaKrVbWloiLS1N5TLdunXDmjVrEBcXByEEYmNjER4ejoKCAqSnF39gHD16FGvXrsXq1aufeTzBwcEwNTWVbra2ts+8LBEREdU8ldoFZmxsjKioKLRq1Uqp/dSpU/D29pZmUMpz8+ZN2NjYIDo6Gu3atZPa58+fj02bNuHixYullnn06BHGjh2LTZs2QQgBS0tLvP/++1i0aBFu3boFAwMDtGjRAqGhoejRowcAwM/P76m7wFTNANna2nIXWAVrOb3N6W3uAqt4LT8jKlfLz4hi/IxQrq3ILrBKBaC+ffvi3r172Lx5M6ytrQEAKSkpeO+992BmZoYdO3Y8dR35+fkwNDTEL7/8gn79+knt/v7+OH36NA4dUj2lCgAFBQW4desWrKyssGrVKkyZMgX37t3Dv//+i9atW0Mul0u1JT/UqqWlhYSEBDRu3PipfeNZYERERDVPRb6/K7ULbMWKFbh//z4cHBzQuHFjNGnSBA0bNsT9+/fx7bffPtM6dHV14ebmhsjISKX2yMhIeHp6lrusjo4OGjRoALlcji1btqBXr17Q0tLCa6+9hrNnz+L06dPSrU+fPujUqRNOnz7NXVtEREQEoJIHQdva2uLkyZOIjIzExYsXIYSAs7MzOnfuXKH1BAYGwtfXF+7u7mjXrh1WrVqFpKQkjBkzBgAQFBSElJQUbNy4EQBw6dIlxMTEwMPDA5mZmVi6dCnOnTuHDRuKD37T19eHi4vyQVK1a9cGgFLtREREpLkqFID+/vtvjBs3DsePH4eJiQm6dOmCLl26AACysrLQvHlzfPfdd8987Z3BgwcjIyMDc+fORWpqKlxcXBAREQF7e3sAQGpqKpKSkqR6hUKBkJAQJCQkQEdHB506dUJ0dDQcHBwqMgwiIiLScBU6Bqhkd9L48eNVPr58+XIcOHDgmY4Bqs54DBAREVHN88KOATpz5gy6d+9e5uNdu3ZFXFxcRVZJRERE9NJVKADdunWr1C+/P05bWxt37twp83EiIiKi6qBCAcjGxgZnz54t8/F///0XVlZWz90pIiIiohepQgGoZ8+emDlzJnJzS1+46NGjR5g1axZ69Sr7ctlERERE1UGFDoK+desWXF1dIZfLMW7cODg6OkImkyE+Ph4rV66EQqHAyZMnS/28RU3Dg6CJiIhqnhf2Y6iWlpaIjo7GJ598gqCgIOmy6jKZDN26dUNoaGiNDz9ERET06qvwhRDt7e0RERGBzMxMXLlyBUIING3aFGZmZi+if0RERERVrlJXggYAMzMztGnTpir7QkRERPRSVOq3wIiIiIhqMgYgIiIi0jgMQERERKRxGICIiIhI4zAAERERkcZhACIiIiKNwwBEREREGocBiIiIiDQOAxARERFpHAYgIiIi0jgMQERERKRxGICIiIhI4zAAERERkcZhACIiIiKNwwBEREREGocBiIiIiDQOAxARERFpHAYgIiIi0jgMQERERKRxGICIiIhI4zAAERERkcZhACIiIiKNwwBEREREGocBiIiIiDQOAxARERFpHAYgIiIi0jgMQERERKRx1B6AQkND0bBhQ+jr68PNzQ1RUVHl1q9cuRJOTk4wMDCAo6MjNm7cqPT46tWr0b59e5iZmcHMzAydO3dGTEzMixwCERER1TBqDUBbt25FQEAApk2bhlOnTqF9+/bo0aMHkpKSVNaHhYUhKCgIs2fPxvnz5zFnzhyMHTsWf/zxh1Rz8OBBDB06FAcOHMCxY8dgZ2eHrl27IiUl5WUNi4iIiKo5mRBCqGvjHh4ecHV1RVhYmNTm5OQEHx8fBAcHl6r39PSEl5cXFi9eLLUFBAQgNjYWR44cUbkNhUIBMzMzrFixAsOHD1dZk5eXh7y8POl+dnY2bG1tkZWVBRMTk8oOj4iIiF6i7OxsmJqaPtP3t9pmgPLz8xEXF4euXbsqtXft2hXR0dEql8nLy4O+vr5Sm4GBAWJiYlBQUKBymZycHBQUFMDc3LzMvgQHB8PU1FS62draVnA0REREVJOoLQClp6dDoVDA0tJSqd3S0hJpaWkql+nWrRvWrFmDuLg4CCEQGxuL8PBwFBQUID09XeUyU6dOhY2NDTp37lxmX4KCgpCVlSXdkpOTKz8wIiIiqva01d0BmUymdF8IUaqtxIwZM5CWloY33ngDQghYWlrCz88PixYtglwuL1W/aNEibN68GQcPHiw1c/Q4PT096OnpPd9AiIiIqMZQ2wyQhYUF5HJ5qdme27dvl5oVKmFgYIDw8HDk5OQgMTERSUlJcHBwgLGxMSwsLJRqlyxZggULFmDfvn1o0aLFCxsHERER1TxqC0C6urpwc3NDZGSkUntkZCQ8PT3LXVZHRwcNGjSAXC7Hli1b0KtXL2hp/W8oixcvxrx58/Dnn3/C3d39hfSfiIiIai617gILDAyEr68v3N3d0a5dO6xatQpJSUkYM2YMgOJjc1JSUqRr/Vy6dAkxMTHw8PBAZmYmli5dinPnzmHDhg3SOhctWoQZM2bgp59+goODgzTDZGRkBCMjo5c/SCIiIqp21BqABg8ejIyMDMydOxepqalwcXFBREQE7O3tAQCpqalK1wRSKBQICQlBQkICdHR00KlTJ0RHR8PBwUGqCQ0NRX5+PgYOHKi0rVmzZmH27NkvY1hERERUzan1OkDVVUWuI0BERETVQ424DhARERGRujAAERERkcZhACIiIiKNwwBEREREGocBiIiIiDQOAxARERFpHAYgIiIi0jgMQERERKRxGICIiIhI4zAAERERkcZhACIiIiKNwwBEREREGocBiIiIiDQOAxARERFpHAYgIiIi0jgMQERERKRxGICIiIhI4zAAERERkcZhACIiIiKNwwBEREREGocBiIiIiDQOAxARERFpHAYgIiIi0jgMQERERKRxGICIiIhI4zAAERERkcZhACIiIiKNwwBEREREGocBiIiIiDQOAxARERFpHAYgIiIi0jgMQERERKRxGICIiIhI4zAAERERkcZRewAKDQ1Fw4YNoa+vDzc3N0RFRZVbv3LlSjg5OcHAwACOjo7YuHFjqZpt27bB2dkZenp6cHZ2xo4dO15U94mIiKgGUmsA2rp1KwICAjBt2jScOnUK7du3R48ePZCUlKSyPiwsDEFBQZg9ezbOnz+POXPmYOzYsfjjjz+kmmPHjmHw4MHw9fXFmTNn4Ovri0GDBuHEiRMva1hERERUzcmEEEJdG/fw8ICrqyvCwsKkNicnJ/j4+CA4OLhUvaenJ7y8vLB48WKpLSAgALGxsThy5AgAYPDgwcjOzsaePXukmu7du8PMzAybN29+pn5lZ2fD1NQUWVlZMDExqezwiIiI6CWqyPe32maA8vPzERcXh65duyq1d+3aFdHR0SqXycvLg76+vlKbgYEBYmJiUFBQAKB4BujJdXbr1q3MdZasNzs7W+lGREREry61BaD09HQoFApYWloqtVtaWiItLU3lMt26dcOaNWsQFxcHIQRiY2MRHh6OgoICpKenAwDS0tIqtE4ACA4OhqmpqXSztbV9ztERERFRdab2g6BlMpnSfSFEqbYSM2bMQI8ePfDGG29AR0cHffv2hZ+fHwBALpdXap0AEBQUhKysLOmWnJxcydEQERFRTaC2AGRhYQG5XF5qZub27dulZnBKGBgYIDw8HDk5OUhMTERSUhIcHBxgbGwMCwsLAED9+vUrtE4A0NPTg4mJidKNiIiIXl1qC0C6urpwc3NDZGSkUntkZCQ8PT3LXVZHRwcNGjSAXC7Hli1b0KtXL2hpFQ+lXbt2pda5b9++p66TiIiINIe2OjceGBgIX19fuLu7o127dli1ahWSkpIwZswYAMW7plJSUqRr/Vy6dAkxMTHw8PBAZmYmli5dinPnzmHDhg3SOv39/dGhQwcsXLgQffv2xc6dO/HXX39JZ4kRERERqTUADR48GBkZGZg7dy5SU1Ph4uKCiIgI2NvbAwBSU1OVrgmkUCgQEhKChIQE6OjooFOnToiOjoaDg4NU4+npiS1btmD69OmYMWMGGjdujK1bt8LDw+NlD4+IiIiqKbVeB6i64nWAiIiIap4acR0gIiIiInVhACIiIiKNwwBEREREGocBiIiIiDQOAxARERFpHAYgIiIi0jgMQERERKRxGICIiIhI4zAAERERkcZhACIiIiKNwwBEREREGocBiIiIiDQOAxARERFpHAYgIiIi0jgMQERERKRxGICIiIhI4zAAERERkcZhACIiIiKNwwBEREREGocBiIiIiDQOAxARERFpHAYgIiIi0jgMQERERKRxGICIiIhI4zAAERERkcZhACIiIiKNwwBEREREGocBiIiIiDQOAxARERFpHAYgIiIi0jgMQERERKRxGICIiIhI4zAAERERkcZhACIiIiKNwwBEREREGkftASg0NBQNGzaEvr4+3NzcEBUVVW79jz/+iJYtW8LQ0BBWVlb44IMPkJGRoVSzbNkyODo6wsDAALa2thg/fjxyc3Nf5DCIiIioBlFrANq6dSsCAgIwbdo0nDp1Cu3bt0ePHj2QlJSksv7IkSMYPnw4PvzwQ5w/fx6//PIL/vnnH4waNUqq+fHHHzF16lTMmjUL8fHxWLt2LbZu3YqgoKCXNSwiIiKq5mRCCKGujXt4eMDV1RVhYWFSm5OTE3x8fBAcHFyqfsmSJQgLC8PVq1eltm+//RaLFi1CcnIyAGDcuHGIj4/H/v37pZoJEyYgJiamzNmlvLw85OXlSfezsrJgZ2eH5ORkmJiYPPc4iYiI6MXLzs6Gra0t7t27B1NT0/KLhZrk5eUJuVwutm/frtT++eefiw4dOqhc5ujRo0JXV1fs3r1bFBUVibS0NNGhQwfx8ccfSzWbN28Wpqam4sSJE0IIIa5evSpee+01ERwcXGZfZs2aJQDwxhtvvPHGG2+vwC05OfmpOUQbapKeng6FQgFLS0uldktLS6SlpalcxtPTEz/++CMGDx6M3NxcFBYWok+fPvj222+lmiFDhuDOnTt48803IYRAYWEhPvnkE0ydOrXMvgQFBSEwMFC6X1RUhLt376JOnTqQyWTPOVIiqk5K/oXIGV6iV48QAvfv34e1tfVTa9UWgEo8GTCEEGWGjgsXLuDzzz/HzJkz0a1bN6SmpmLSpEkYM2YM1q5dCwA4ePAg5s+fj9DQUHh4eODKlSvw9/eHlZUVZsyYoXK9enp60NPTU2qrXbv28w+OiKotExMTBiCiV9BTd339P7UFIAsLC8jl8lKzPbdv3y41K1QiODgYXl5emDRpEgCgRYsWqFWrFtq3b48vv/xSCjm+vr7SgdGvv/46Hj58iI8++gjTpk2DlpbaT3wjIiIiNVNbGtDV1YWbmxsiIyOV2iMjI+Hp6alymZycnFIBRi6XAyieOSqvRggh1RAREZFmU+susMDAQPj6+sLd3R3t2rXDqlWrkJSUhDFjxgAoPjYnJSUFGzduBAD07t0bo0ePRlhYmLQLLCAgAG3btpX29/Xu3RtLly5F69atpV1gM2bMQJ8+faSwRESaS09PD7NmzSq125uINItaT4MHii+EuGjRIqSmpsLFxQVff/01OnToAADw8/NDYmIiDh48KNV/++23+O6773D9+nXUrl0bb731FhYuXAgbGxsAQGFhIebPn49NmzYhJSUFdevWRe/evTF//nwe10NEREQAqkEAIiIiInrZeEQwERERaRwGICIiItI4DEBERESkcRiAiIiISOMwABHRK+fy5cvIyclRdzeIqBpjACKiV8atW7fw3nvvoUOHDjh//ry6u0NE1RgDEBG9EgIDA2FjY4Pjx48jIyMDdevWVXeXiKgaYwAiohptw4YNMDc3x/79+xEdHY25c+fCyckJ9evXV3fXiKgaU/uvwRMRVUZmZib69euHmJgYLF++XPoB5D///BNGRkbQ19dXcw+JqDpjACKiGqWwsBDa2trIz8/HuHHj0LNnTxgaGkqPHzt2DM2aNQMAKBQK/gYgEanEXWBEVGMEBwdj0KBBOH/+PCwtLTFw4EAYGhqi5Bd98vPzcePGDbi6ugIAZDKZOrtLRNUYAxARVXunT5+Gi4sLNm7ciO7du8PExAQKhUJ6XCaTQaFQICcnBxkZGWjYsCEAQEuLH3FEpBp3gRFRtffzzz+jZcuW+PHHHwEARUVFSuGmqKgIcrkcV65cwYMHD+Dm5qa0/OHDh3H+/Hl88sknL7XfRFR98Z9HRFStpaWlYc2aNQgICAAATJw4ER999BGmTp2KuLg4AP+b6bl27RpsbGxgYGAAALhx4wb69u2Lt956C48ePVJL/4moemIAIqJq5ezZs9L/lxzbY2dnhwsXLuCdd95BXFwcbGxssG3bNowaNQr79u2T6pOSklC/fn3o6upi6tSpaNKkCYDiEBUYGPhyB0JE1RoDEBFVCw8ePEDbtm3Rs2dPHDt2DEDxsT15eXkoKirC3r17oa+vjy1btmDOnDnYu3cvmjdvjmnTpknriImJQUJCAhwcHLBjxw4cPHgQO3fuhIWFhbqGRUTVFAMQEVULN27cQGJiIl5//XVs2LBBare3t4eHhwe2bNkCQ0NDWFpaAgAaNWqEAQMGoLCwEKdOnQIAyOVyyGQyLF68GAkJCfDy8lLLWIio+mMAIqJqQVdXF+7u7mjRogXOnDmDbdu2SY99+eWXMDAwQGJiIhITE6X2wsJCJCUlSTM8EyZMQFpaGkaMGPGyu09ENQwDEBFVCwcOHICVlRUmTZqEOnXqSGd8AUCdOnUwf/58JCYmIiQkBBkZGcjKysLRo0fRq1cvKQC5u7urq/tEVMPwNHgiUishBGQyGfT19SGXy1GnTh28++67WLt2LcaNG4ebN29iw4YNCAgIQFFRERYvXoxDhw7h3r17MDIywk8//SSd9UVE9KxkouQ0CyKiF6gk6JTF398fRUVF+Pbbb3H58mV07twZqampeOedd7Bt2zbpVPfr16/j6tWrePDgAXx8fF5S74noVcNdYET0QpUcs1MSfkr+zfXjjz8iJiZGqtPT08Obb76JadOmwcXFBRYWFnBxcYGLi4sUfoQQaNiwITp37szwQ0TPhQGIiF6IrVu3omnTpujduze6du2KX375BUBxELp06RJ8fX2xa9cu5ObmAgCuXLmCoUOH4rfffsPPP/+MEydOoEuXLti5cyf2798vLUtEVBV4DBARVbnt27djwoQJmD59OiwsLPDnn39i2LBh0NLSQs+ePdGsWTOEh4ejTZs20NfXBwBMnz4d77zzDgYMGIDatWsDAHx8fHD37l1YWVmpcTRE9CriMUBEVGVKfqNr7NixuHbtGvbs2SM95uvri/j4eHz11Vfo3LmzGntJRMRdYERUBbKzswH87ze5zpw5g6ZNmwIA8vLyAABff/01cnNz8ccffyAzM1Np+aSkpJfYWyIiBiAieg4nT55E8+bNsXLlSqUfG+3cuTN27twJoPjg5sLCQlhYWODDDz/E77//juTkZKl25syZ+OCDD3DmzJmX3n8i0lwMQERUaQcOHEB8fDx++OEHJCQkSO2enp7Q1tbGihUrABTvGgOA8ePH4+7duzh58qRU27ZtWzx8+BC1atV6uZ0nIo3GAEREFVZy6OCjR4+wbt06PHjwACtXrkRWVhYAoEWLFujevTvCwsKQmZkJXV1dKBQKFBQUwNHRUSks9erVC8ePH5d+uZ2I6GVgACKiCis5HT0mJga1a9fGypUrsX79epw4cQIAUL9+fbz//vvQ1taGn58f8vPzIZfLkZSUhMzMTHTv3l2d3Sci4mnwRFRxQggoFApkZ2fDysoKbdu2Rdu2bbFixQpkZ2fjv//+Q0BAAMLCwtCnTx+4urrC3d0dkZGRaNmyJVxcXJ56ZWgioheJM0BE9FQlx/A8fl9bWxtCCOlChrNnz8auXbswbNgwPHjwAEVFRfD09MS+ffvwySefQC6XY/78+YiIiECdOnUYfohIrTgDREQqPT5DU3J6e1RUFNq3bw+5XI6UlBTI5XK0aNECkyZNwjfffANbW1vI5XL4+vpKy7i6usLV1VVt4yAiUoUzQEQk2bZtGxYvXozCwsJSMzTLly+Ht7e3dAq7EALXr1+Hubk5IiIisGfPHpw/fx537txBaGioNDNERFQdcQaIiJCcnIxJkybh559/houLC15//XV0795daRaoX79+SEpKks4A09fXx6BBg+Do6IhBgwbByMgIABAYGIiDBw+isLBQbeMhInoa/hQGEWHTpk3YtGkTfH19sXr1ajg7O2P+/PmoU6cOFAoF5HK5yuXy8vKgp6cHADyomYhqFM4AERE6deqEevXqoVu3brh37x42btyIXbt2YcSIEWWGHwBS+AH+d2o8gxAR1QScASLSMEePHoVCoUDDhg1hY2MDLS0t6UdMASAnJwcDBw5ErVq1sGDBAjRt2lTp8fv372PEiBHo1q0bPv74Y3UOhYio0ngQNJGGuHjxIry8vDB06FB8+umn6NixI8LDwwH87yyvwsJCGBoaYvTo0bh69Sp+++03pccBQEdHBzo6Oti8eTMUCsVLHwcRUVVgACLSAFFRUXj33Xfx+uuv49ixY/jtt9/QsWNH/PTTT4iPj5fqtLWL94r369cPLVq0QGRkJI4fPw4AUp2+vj6+++47HDx4sNzdY0RE1Rl3gRG9wr7//nvY29vj6tWruHz5Mr744gvUq1cPAHDo0CEMGzYM0dHRsLe3l5YpOeg5Li4O48aNQ8uWLZGTk4OtW7fi2LFjvKYPEb0SeBA00Svo999/x6effoqCggIcP34clpaW6N+/vxR+AMDe3h5aWlqlrtdTMqvj5uYGLS0trFq1Cm3atEFUVBTDDxG9MhiAiF4hiYmJCAgIwK5du7B8+XKMHj0aOjo6SjUlBzQfPXoUenp6SrM/Ja5cuYI33ngD2tra+PnnnzFw4MCXNQQiopeCxwARvQIUCgU++ugjNGrUCJGRkejfvz8+/fRT6OjolDpQueSA5oMHD6Jbt27Q19cvtb4mTZogKCgIaWlpDD9E9EpiACKq4c6cOQMdHR2cO3cO8fHx+Prrr5GcnCyd4VXWNXnOnTsHLy8vAEBBQQGWL1+Os2fPSo9PmDDhxXeeiEhNGICIaqgzZ87g2rVrqFu3Lvbv34/o6Gg4OjqiV69esLOzw7Zt23Dz5k3pOj+PS0xMREpKCt544w1s374dDRo0QFhYGHR1ddU0GiKil4sBiKiGuXXrFvr06YO3334be/fuhZmZGTp16gSgeFeYtbU1+vXrh8zMzFLX+Smxd+9eZGZmol+/fvD19cX06dMRHx8PR0fHlz4eIiJ14EHQRDVIcnIy+vfvD2traxw7dgz169eHgYGB9PjjP1x6+PBhREZGomfPnnB1dVW6mvN///2H3NxcdO7cGadOnSoVkIiIXnX81COqQSIiImBubo6dO3eiadOmSExMRFJSkvTL61paWigsLISenh4GDx4MLS0tpVmgkl1hffv2RXJyMkJCQhh+iEgj8ZOPqAYouV7pv//+iwYNGuDu3bvo1KkTBg8ejHbt2uG9995DVFQUgP/NAnl7e8Pb2xtnzpyRftKiZD3u7u6oX7/+yx8IEVE1wQBEVA1lZ2fj+PHjSElJAVAcaoqKipCZmYlatWph6tSpaNq0KbZt24Zly5YhOzsb06ZNQ1paGuRyOQoKCgAAQ4YMQWFhIXbu3Cld4ZmIiBiAiKqd4OBg2NraYvTo0XB2dsY333yDpKQkaGlpoW3btlixYgX27t2LTz/9FE5OTnj33Xfx2WefQaFQYOvWrQAgXfzwtddeQ0hICL7//nuGHyKixzAAEVUje/bswaZNm7Bhwwb8/vvvmDJlClavXo3Zs2cDAD777DM4OTkhMzNT6didLl264MGDB0rX/CnZ3eXp6cnT24mInsAARFSN7N27F/r6+vDx8UHDhg3xxRdfYMyYMYiOjpZmcaZNm4YHDx5gz5490kHNeXl5kMlkMDMzk9ZV1gUQiYiIAYio2igqKkJ+fj4cHR2Rl5cntQ8YMAAdO3ZEaGgo7t+/j2HDhmHo0KHYsGEDhgwZgl27dmHIkCHQ1tbGW2+9pcYREBHVHAxARNWAEAJaWlqws7PD0aNHkZqaKj1mZWWFd955B7q6uvjhhx8AAKGhoQgKCsKtW7cwb948GBkZITIyEjY2NuoaAhFRjSITJQcKENFLJYSQdlOVnKGVm5sLS0tLTJ48GdOmTZNq79+/j86dO6Nfv36YOnWq1J6fn4+HDx8q7foiIqKn4wwQ0Utw48YNzJ07Fxs2bMCJEycA/O8g5cLCQukMLX19fcycORMLFy5EbGystLyxsTHy8/ORnJystF5dXV2GHyKiSmAAInrBpk6dCmdnZxw/fhxfffUVBg4ciAsXLkBLSwtCCGhra0MIgSlTpuCHH37AhAkT0KxZM0ydOhUREREAgJMnT0IIAR8fH/UOhojoFcEARPQCbd++HX/99Rd27dqFiIgIbN68Gba2tvjjjz8AFJ+ptWHDBlhYWGDfvn1o3rw5AGDTpk0wMTFBv3790K1bN7Rv3x5OTk7w8vJS53CIiF4ZPAaIqAo9flwPAIwYMQLXrl2TfqYCADp27Ijly5ejRYsWEEJg7ty5sLS0xOjRoyGXy6V1ZGdn48SJE7h06RJatWrF8ENEVIUYgIiqyKNHj6ClpQU9PT0Axcf2LFq0CN9//z02b94MKysr+Pv748iRI2jTpg2aNWuGhQsXwtDQUM09JyLSPNwFRlQFgoKC8Oabb6JXr15Yvnw5srKyoK2tDR8fH7Rr1w7BwcFo0qQJCgsL8csvv6Bbt27Ys2cPPvroIwCQLmhIREQvB2eAiJ5Dfn4+3nvvPVy4cAEzZ85EREQEYmNjYW9vLx3ALITA5s2b8eOPP2LLli0wNjYGAPz+++8YOnQobty4AQsLC3UOg4hI43AGiOg5XL16FWfOnMGyZcswePBgbNiwAatWrcLBgwexePFiKBQKyGQynDt3Dnp6elL4AYDr16+jQYMGyMnJUeMIiIg0EwMQ0XN49OgRrly5Ajc3NwDFsz1eXl6YOXMmgoODce3aNQBAbm4usrOzERERAYVCgfj4eGzfvh1dunSBnZ2dOodARKSRGICInoOWlhacnZ3x008/KbVPmDABtWvXRmhoKABgyJAhMDIyQr9+/dCzZ0+0bdsWTZo0wZIlS9TRbSIijcdjgIieQ2ZmJkaOHAk9PT18/fXXsLKyQmFhIbS1tbF06VIsWbIEycnJkMvlSE1NxYkTJ5CSkoK33noLTk5O6u4+EZHG4gwQURnOnz+PyZMn49KlS6UeKywsBACYmZmhd+/euHjxIn7++WcAgLa2NgDA1NQU5ubm0s9XWFlZwcfHB2PHjmX4ISJSMwYgoifk5+fjgw8+wOuvv47c3Fw4ODhIj5VMmGprayM3NxdbtmzByJEj0apVK2zduhUHDhyQav/77z/UrVtXaXkiIqoeGICIHhMeHg4LCwtcunQJZ86cwfLly6GrqwtA+SrPy5cvh42NDbZs2QIACAwMRKNGjdC9e3d8+umn+PjjjxESEoLBgwdLyxIRUfXBY4CIHuPl5YXMzExER0ejdu3aOHnyJG7duoXGjRvDzs4O+vr6WLFiBUJCQjB//nwMGTIEWlrF/44QQuCrr77C9evXkZKSgmnTpsHT01PNIyIiIlUYgIgAKBQKyOVyHDt2DO+//z5GjBiBuLg4nDt3DgYGBkhPT0eHDh3w888/o7CwEHl5eahVq5a0/JO/AUZERNUbAxBprFWrVkEmk6FZs2bw9vaW2j/88EP89NNPGDJkCAICAqClpYUbN27g3XffxdSpUzFr1iw19pqIiKoCAxBpnM2bNyMgIACNGzfGo0ePkJKSAn9/f0ybNg0AkJ6ejm+++QaffPIJrK2tpeWWLl2KBQsWIDU1FTo6OurqPhERVQEeBE0a5aeffsLChQsxd+5cREdHY/fu3dJVm+/fvw8AsLCwQFBQkFL4AQBra2sUFRUhISFBHV0nIqIqxABEGqFkorOgoAAeHh4YPnw4gOJQ07p1a9jY2CA+Pl6qNzQ0LLWOw4cPo1OnTnBxcXk5nSYioheGAYheaSdPnsS9e/ekA5R9fHwQGhoKAwMDqcbIyAjZ2dlo2rRpqeUTExNx9epVjBo1Crt375aCE/ccExHVbAxA9Eratm0bbG1tMWjQILRs2RIzZ87ErVu3YGpqCrlcjqKiIqn277//RuPGjWFmZoaCggKpPSEhAUuWLIGnpyeuXr2KyMhI9O3bFwB4xhcRUQ2nre4OEFW12NhYTJ8+HRMnTkSnTp1w9OhRzJo1C+np6fjyyy9hbm4OANJvdkVFRaFVq1YAoHRws62tLfr27YvBgwejffv26hgKERG9IJwBoldGyW6p2NhYPHjwAB988AFatGiBTz75BLNmzcKpU6ekX2fX0tKClpYWhBD4999/0aNHDwDApUuXMHToUCQnJ8PQ0BBdunRh+CEiegUxANEro2S31PXr19GsWTPpR0kBwM/PD25ubtizZw/Onz8PoDgE/fPPPzA0NISrqysCAgLQokULZGRkoF69emoZAxERvRwMQFRjRUZG4vPPP8c333yDmJgYqd3LywvR0dFIS0sDUHyV51q1aqFv376QyWTYt2+fVBsREYFz587B0dERkZGROHr0KPbt2wc9Pb2XPh4iInp5GICoxklNTUXv3r3x/vvv4+7du1i7di26du0qhaCuXbvCwcEBCxcuBPC/maEuXbpAS0sLV65ckdalo6MDCwsLrF+/HufPn4ebm9vLHxAREb10vBI01Sg5OTn49NNPkZubi+DgYDRs2BAA0LZtWzRv3hzr1q2DQqHATz/9BD8/P0RFRSn9IOn777+PlJQUHDhwAABw584d1K1bVy1jISIi9eEMENUohoaG0NPTg5+fHxo2bIjCwkIAQK9evaQLGcrlcgwaNAh9+/bFqFGjcOjQIQghkJaWhsuXL+P999+X1sfwQ0SkmTgDRDVOQUGBdLp6ya+w+/r6wsDAAKtWrZLacnNz0aNHD1y4cAGtWrXCuXPnYGdnh59//hm2trZqHgUREakTAxC9Ejp06ICRI0fCz88PQggUFRVBLpfj1q1b+Pfff/HPP//AwcEBw4YNU3dXiYioGmAAohrv2rVr8PT0xO7du6WDmPPz86Grq6vmnhERUXXFY4CoxirJ7keOHIGRkZEUfubMmQN/f3/cvn1bnd0jIqJqjD+FQTVWyentMTExGDBgACIjI/HRRx8hJycHmzZt4sUMiYioTNwFRjVabm4uXn/9dVy9ehW6urqYM2cOpkyZou5uERFRNccARDVely5d0LRpUyxduhT6+vrq7g4REdUADEBU4ykUCsjlcnV3g4iIahAGICIiItI4PAuMiIiINA4DEBEREWkcBiAiIiLSOAxAREREpHEYgIiIiEjjMAARERGRxmEAIiIiIo3DAEREREQahwGIiIiINA4DEBEREWmc/wN7vZ8qgHbOuAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import  matplotlib.pyplot as plt\n",
    "# Populate the data for the x and y axis\n",
    "x = []\n",
    "y = []\n",
    "for obj in json_data:\n",
    "    inference, timestamp = simple_getter(obj)\n",
    "    \n",
    "    y.append(max(inference))\n",
    "    x.append(timestamp)\n",
    "\n",
    "# Todo: here is an visualization example, take some time to build another visual that helps monitor the result\n",
    "# Plot the data\n",
    "plt.scatter(x, y, c=['r' if k<.94 else 'b' for k in y ])\n",
    "plt.axhline(y=0.94, color='g', linestyle='--')\n",
    "plt.ylim(bottom=.88)\n",
    "\n",
    "# Add labels\n",
    "plt.ylabel(\"Confidence\")\n",
    "plt.suptitle(\"Observed Recent Inferences\", size=14)\n",
    "plt.title(\"Pictured with confidence threshold for production use\", size=10)\n",
    "\n",
    "# Give it some pizzaz!\n",
    "plt.style.use(\"Solarize_Light2\")\n",
    "plt.gcf().autofmt_xdate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo: build your own visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.savefig('model_monitoring_visualization.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations!\n",
    "\n",
    "You've reached the end of the project. In this project you created an event-drivent ML workflow that can be incorporated into the Scones Unlimited production architecture. You used the SageMaker Estimator API to deploy your SageMaker Model and Endpoint, and you used AWS Lambda and Step Functions to orchestrate your ML workflow. Using SageMaker Model Monitor, you instrumented and observed your Endpoint, and at the end of the project you built a visualization to help stakeholders understand the performance of the Endpoint over time. If you're up for it, you can even go further with these stretch goals:\n",
    "\n",
    "* Extend your workflow to incorporate more classes: the CIFAR dataset includes other vehicles that Scones Unlimited can identify with this model.\n",
    "* Modify your event driven workflow: can you rewrite your Lambda functions so that the workflow can process multiple image inputs in parallel? Can the Step Function \"fan out\" to accomodate this new workflow?\n",
    "* Consider the test data generator we provided for you. Can we use it to create a \"dummy data\" generator, to simulate a continuous stream of input data? Or a big paralell load of data?\n",
    "* What if we want to get notified every time our step function errors out? Can we use the Step Functions visual editor in conjunction with a service like SNS to accomplish this? Try it out!\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
